{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrea.feher\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import win32com.client\n",
    "import sys\n",
    "import re\n",
    "import pandas\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "import pandas as pd\n",
    "from pandas.compat import StringIO\n",
    "from langdetect import detect\n",
    "import io\n",
    "import os\n",
    "import unicodecsv as csv\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import codecs\n",
    "import pickle\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import mpld3\n",
    "import uuid\n",
    "import datetime\n",
    "from gensim.summarization import keywords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND TESTING\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "rubbish = [\"intended recipient\", \"designated recipient\", \"consider the environmental\", \"may contain confidential\", \n",
    "           \"may be confidential\", \"received this in error\", \"communication in error\", \"privileged and confidential\",\n",
    "           \"environmental impact\", \"consider the environment\",  \"before printing\", \"Before printing\", \"Twitter\",\n",
    "           \"from an EXTERNAL SENDER\", \"Sent: \", \"Cc: \", \"LinkedIn\", \"Sent from my iPhone\", \"Thank you in advance.\", \"RE: \",\n",
    "          'vriendelijke', 'Euregio Digital Delivery Center', 'information that is confidential', 'for the person or entity']\n",
    "\n",
    "\n",
    "\n",
    "def import_train_test (output_folder, root_folder, folder, year, month, day, hour, minute, second, classes, spamm, rubbish = rubbish):\n",
    "    if output_folder[-1] != \"/\":\n",
    "        output_file = output_folder + '/jk_sct.csv'\n",
    "    else:\n",
    "        output_file = output_folder + 'jk_sct.csv'\n",
    "    importmails(output_file, root_folder, folder, year, month, day, hour, minute, second)\n",
    "    if output_folder[-1] != \"/\":\n",
    "        training_data = output_folder + '/jk_sct_train.csv'\n",
    "    else:\n",
    "        training_data = output_folder + 'jk_sct_train.csv'\n",
    "    cleanmails(training_data, 'en')\n",
    "    if output_folder[-1] != \"/\":\n",
    "        language_filtered = output_folder + '/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv'\n",
    "    else:\n",
    "        language_filtered = output_folder + 'Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv'\n",
    "    cluster(language_filtered, classes)\n",
    "    if output_folder[-1] != \"/\":\n",
    "        labeled = output_folder + '/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv'\n",
    "    else:\n",
    "        labeled = output_folder + 'Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv'\n",
    "    addkeywords (labeled, classes, spamm)\n",
    "    trainsvm (labeled)\n",
    "    if output_folder[-1] != \"/\":\n",
    "        testing_data = output_folder + '/jk_sct_test.csv'\n",
    "    else:\n",
    "        testing_data = output_folder + 'jk_sct_test.csv'\n",
    "    testsvm(testing_data)\n",
    "    if output_folder[-1] != \"/\":\n",
    "        removepath = output_folder + '/Python Notebooks and Scripts/emails/'\n",
    "    else:\n",
    "        removepath = output_folder + 'Python Notebooks and Scripts/emails/'\n",
    "    remove_files(removepath)\n",
    "    \n",
    "def importmails(output_file1, root_folder, folder, year, month, day, hour, minute, second):\n",
    "    print()\n",
    "    print(\"************************\")\n",
    "    print()\n",
    "    print('... Initialized ...')\n",
    "    print()\n",
    "    print(\"************************\")\n",
    "    print()\n",
    "    print('... Accessing mailbox: ' + root_folder.upper())\n",
    "    output_file = open(output_file1,'wb')    \n",
    "    output_writer = csv.writer(output_file, delimiter = \",\", encoding='latin2', errors = 'ignore')\n",
    "    outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "    root_folder = outlook.Folders(root_folder)\n",
    "    folder = root_folder.Folders(folder)\n",
    "    ssbox = root_folder.Folders[1]\n",
    "    counter = 0\n",
    "    for folder in ssbox.Folders:\n",
    "        counter += 1\n",
    "    if counter < 2:\n",
    "        inbox = outlook.GetDefaultFolder(6)\n",
    "        messages = inbox.Items\n",
    "        messages = messages.Restrict(\"[ReceivedTime] >= '\" +datetime.datetime(year, month, day, hour, minute, second).strftime('%d/%m/%Y, %H:%M %p')+\"'\")\n",
    "        message = messages.GetFirst()\n",
    "        while message:\n",
    "            try:\n",
    "                if message.SenderEmailType=='EX':\n",
    "                    sender = re.sub(r'.*-', '', message.SenderEmailAddress).lower()+'@accenture.com'\n",
    "                else:\n",
    "                    sender = message.SenderEmailAddress\n",
    "                output_writer.writerow([\n",
    "                    message.subject,\n",
    "                    sender,\n",
    "                    message.Body])\n",
    "            except Exception as e:\n",
    "                    ()\n",
    "            message = messages.GetNext()\n",
    "        output_file.close()\n",
    "    else:\n",
    "        for folder in ssbox.Folders:    \n",
    "            print(folder)\n",
    "            messages = folder.Items\n",
    "            messages = messages.Restrict(\"[ReceivedTime] >= '\" +datetime.datetime(year, month, day, hour, minute, second).strftime('%d/%m/%Y, %H:%M %p')+\"'\")\n",
    "            message = messages.GetFirst()\n",
    "            print('-------------')\n",
    "            print(str(len(list(messages)))+' messages in folder ')\n",
    "            while message:\n",
    "                try:\n",
    "                    if message.SenderEmailType=='EX':\n",
    "                        sender = re.sub(r'.*-', '', message.SenderEmailAddress).lower()+'@accenture.com'\n",
    "                    else:\n",
    "                        sender = message.SenderEmailAddress\n",
    "                    output_writer.writerow([\n",
    "                        message.subject,\n",
    "                        sender,\n",
    "                        message.Body])\n",
    "                except Exception as e:\n",
    "                    ()\n",
    "                message = messages.GetNext()\n",
    "        output_file.close()\n",
    "    split(output_file1)\n",
    "\n",
    "def split(output_file1):\n",
    "    df4 = pandas.read_csv(output_file1, header=None, encoding = \"latin2\")\n",
    "    train=df4.sample(frac=0.7,random_state=1993)\n",
    "    test=df4.drop(train.index)\n",
    "    test2=test.sample(frac=0.99,random_state=1993)\n",
    "    test3=test.drop(test2.index)\n",
    "    splitted = output_file1.split('/')\n",
    "    root = output_file1.strip(splitted[-1])\n",
    "    train_name = root + \"jk_sct_train.csv\"\n",
    "    test_name = root + \"jk_sct_test.csv\"\n",
    "    testt_name = root + \"jk_sct_test1.csv\"\n",
    "    train.to_csv(train_name,header=None, index=False)\n",
    "    test2.to_csv(test_name,header=None, index=False)\n",
    "    test3.to_csv(testt_name,header=None, index=False)\n",
    "\n",
    "def remove_files (folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "def removepersonalnames(text):\n",
    "    document_big = nlp_big(text)\n",
    "    entities_big = [e.string for e in document_big.ents if 'PERSON'==e.label_] \n",
    "    entities_big = list(entities_big) \n",
    "    for i in entities_big:\n",
    "        if i in text:\n",
    "            text = text.replace(i, '')\n",
    "    return text\n",
    "\n",
    "def cleanmails(data, lang):\n",
    "    try:\n",
    "        splitted = data.split('/')\n",
    "        output_folder = data.strip(splitted[-1])\n",
    "        removalpath = output_folder + 'Python Notebooks and Scripts/emails/'\n",
    "        print( )\n",
    "        print('... Initiating the cleaning process ...')\n",
    "        df = pandas.read_csv(data, header=None, encoding = \"latin2\")\n",
    "        sep1 = '< > > wrote:'\n",
    "        sep2 = ' > het volgende geschreven:'\n",
    "        oldreplacebody = ('\\r', '\\t','\\n \\n','\\n\\n\\n\\n','\\n\\n\\n','\\n\\n')\n",
    "        newreplacebody = ('','',\"\\n\",\"\\n\",\"\\n\",\"\\n\")\n",
    "        oldreplacesub = ('[External]', 'FW: ', 'Fw: ', 'Fwd: ', 'RE: ', 'Re: ')\n",
    "        newreplacesub = ('','','', '','', '')\n",
    "        for i,j in df.iterrows():\n",
    "            j[2] = str(j[2])\n",
    "            j[2] = re.sub('<[^>]+>', '< >', j[2], flags=re.MULTILINE)\n",
    "            j[2] = re.sub('To:[^>]+\\n', '', j[2], flags=re.MULTILINE)\n",
    "            j[2] = j[2].split(sep1, 1)[0]\n",
    "            j[2] = j[2].split(sep2, 1)[0]\n",
    "            for check, rep in zip(oldreplacebody, newreplacebody):\n",
    "                j[2] = j[2].replace(check, rep)\n",
    "            j[0] = str(j[0])    \n",
    "            for check, rep in zip(oldreplacesub, newreplacesub):\n",
    "                j[0] = j[0].replace(check, rep)\n",
    "        df.rename(columns={0:'subject', 1:'sender_address', 2: \"body\"}, inplace=True)\n",
    "        corpus, ID = [], []\n",
    "        global cellist\n",
    "        cellist = []\n",
    "        for i,j in df.iterrows():\n",
    "            ID.append(str(uuid.uuid4()))   \n",
    "        df['ID'] = ID\n",
    "        for i,j in df.iterrows():\n",
    "            corpus.append(j['body'])\n",
    "        length = len(df)\n",
    "        if len(df) == len(corpus):\n",
    "            print('The total number of files to clean is ', str(len(df)))\n",
    "        global nlp_big\n",
    "        nlp_big =  spacy.load('en_core_web_md')\n",
    "        if length > 10:\n",
    "            if length > 1000:\n",
    "                counter = round(length/50)\n",
    "            else:\n",
    "                counter = round(length/10)\n",
    "            for r in range(0, len(corpus)):\n",
    "    #            corpus[r] = removepersonalnames(corpus[r])\n",
    "                convert(corpus[r], output_folder)\n",
    "                perc = r * 100/length\n",
    "                rounded = \"{0:.2f}\".format(perc)\n",
    "                if r % counter == 0:\n",
    "                    print('... Approximately', rounded, ' % of your files are clean, currently cleaning file no. ', len(cellist), '...')\n",
    "        else:\n",
    "                counter = length\n",
    "                for r in range(0, len(corpus)):\n",
    "    #                corpus[r] = removepersonalnames(corpus[r])\n",
    "                    convert(corpus[r], output_folder)\n",
    "        distilled_read = removalpath + 'jk2.csv'\n",
    "        df3 = pandas.read_csv(distilled_read, header=None, encoding = \"latin2\")\n",
    "        print('... Filtering emails in English ...')\n",
    "        df.index = df3.index\n",
    "        df3['sender_address'] = df['sender_address']\n",
    "        df3['subject'] = df['subject']\n",
    "        df3['body'] = df['body']\n",
    "        df3['ID'] = df['ID']\n",
    "        df3.rename(columns={0:'corpus'}, inplace=True)\n",
    "        df3['corpus'] = df3['corpus'].map(str) + '\\n' + df3['subject'] + \" \" + df3['subject'] + \" \" + df3['subject']\n",
    "        lang1 = []\n",
    "        for i,j in df3.iterrows():\n",
    "            j['corpus'] = str(j['corpus'])\n",
    "            if len(j['corpus']) > 100:\n",
    "                l = detect(j['corpus'])\n",
    "            else:\n",
    "                l = \"email too short\"\n",
    "            lang1.append(l)\n",
    "        df3['language'] = lang1\n",
    "        df4 = df3.loc[df3.language == str(lang)]\n",
    "        language_filtered = removalpath + 'jk_sct_eng_complete.csv'\n",
    "        df4.to_csv(language_filtered, index=False)\n",
    "        print ()\n",
    "        print ('CLEANING COMPLETE')\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files(removalpath)\n",
    "\n",
    "def convert(text, output_folder, threshold=.95):\n",
    "    pos_tagger = English()\n",
    "    original_email = text\n",
    "    sentences = _corpus2sentences(original_email)\n",
    "    _generate_text(sentences, output_folder)\n",
    "\n",
    "def _corpus2sentences(text):\n",
    "    return text.strip().split('\\n')\n",
    "\n",
    "def _generate_text(sentences, output_folder, threshold=0.95):\n",
    "    sen = '\\n'\n",
    "    clean = \"\"\n",
    "    for sentence in sentences: \n",
    "        if _prob_block(sentence, nlp_big) < threshold:\n",
    "            if not any(r in sentence for r in rubbish):\n",
    "                if clean == '':\n",
    "                    clean = sen + sentence\n",
    "                else:\n",
    "                    clean = clean + sen + sentence\n",
    "    _generate_list(clean, output_folder)\n",
    "\n",
    "def _generate_list(clean, output_folder):\n",
    "    cellist.append(clean)\n",
    "    if output_folder[-1] != \"/\":\n",
    "        distilled = output_folder + '/Python Notebooks and Scripts/emails/jk2.csv'\n",
    "    else:\n",
    "        distilled = output_folder + 'Python Notebooks and Scripts/emails/jk2.csv'\n",
    "    with open(distilled, 'wb') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for h in cellist:\n",
    "            wr.writerow([h])\n",
    "    \n",
    "def _prob_block(sentence, pos_tagger):   \n",
    "    doc = pos_tagger(sentence)\n",
    "    verb_count = np.sum([token.pos_ != \"VERB\" for token in doc])\n",
    "    length = float(len(doc))\n",
    "    thing = 0\n",
    "    if verb_count > 0:\n",
    "        thing = verb_count/length\n",
    "    else:\n",
    "        thing == 0.0\n",
    "    return thing\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def make_corpus(dataframe):\n",
    "    corpus = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        corpus.append(j['corpus'])\n",
    "    return corpus\n",
    "\n",
    "def make_subject(dataframe):\n",
    "    subject = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        subject.append(j['subject'])\n",
    "    return subject\n",
    "\n",
    "def make_ID(dataframe):\n",
    "    ID = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        ID.append(j['ID'])\n",
    "    return ID\n",
    "\n",
    "def unique(element, listt):\n",
    "    newlist = []\n",
    "    for l in listt:\n",
    "        newlist.append(l)\n",
    "    return listt.count(element) == 1\n",
    "\n",
    "def cluster (filename, dictionary):\n",
    "    try:\n",
    "        removalpath = filename.replace(\"/jk_sct_eng_complete.csv\", '/')\n",
    "        print()\n",
    "        print('... Clustering into your ' + str(len(dictionary)) + ' topics according to the keywords given ...')\n",
    "        df = pd.read_csv(filename, encoding = \"latin2\")\n",
    "        corpus = make_corpus(df)\n",
    "        subject = make_subject(df)\n",
    "        ID = make_ID(df)\n",
    "        topicc = []\n",
    "        for j in range(0, len(corpus)):\n",
    "            corpus[j] = corpus[j].lower()\n",
    "            readlines = corpus[j].strip().split('\\n')\n",
    "            last = readlines[-1]\n",
    "            topic = []\n",
    "            for topics in dictionary.keys():\n",
    "                topic.append(topics)\n",
    "                top, top2 = [], []\n",
    "                for values in dictionary.values():\n",
    "                    counter = 0\n",
    "                    for v in values:\n",
    "                        if values.index(v) <= 2:\n",
    "                            counter += (last.count(v)) * 3\n",
    "                    top.append(counter)\n",
    "            if not all(x == 0 for x in top) and top[0] >= 2:\n",
    "                topicc.append(topic[0])\n",
    "            if not all(x == 0 for x in top) and top[0] == 0:\n",
    "                highest_index = top.index(max(top))\n",
    "                if unique(max(top), top) == True:\n",
    "                    topicc.append(topic[highest_index])\n",
    "                else:\n",
    "                    for values in dictionary.values():\n",
    "                        counter = 0\n",
    "                        for v in values:\n",
    "                            if values.index(v) == 0:\n",
    "                                counter += (corpus[j].count(v)) * 3\n",
    "                            if values.index(v) >= 1 and values.index(v) <= 2:\n",
    "                                counter += (corpus[j].count(v)) * 2\n",
    "                            else:\n",
    "                                counter += corpus[j].count(v)\n",
    "                        top2.append(counter)\n",
    "                    if all(x == 0 for x in top2) :\n",
    "                        topicc.append('other')\n",
    "                    else:\n",
    "                        highest_index = top2.index(max(top2))\n",
    "                        if max(top2) > 2:\n",
    "                            topicc.append(topic[highest_index])\n",
    "                        else:\n",
    "                            topicc.append('other')\n",
    "            if all(x == 0 for x in top):\n",
    "                for values in dictionary.values():\n",
    "                    counter = 0\n",
    "                    for v in values:\n",
    "                        if values.index(v) == 0:\n",
    "                            counter += (corpus[j].count(v)) * 3\n",
    "                        if values.index(v) >= 1 and values.index(v) <= 2:\n",
    "                            counter += (corpus[j].count(v)) * 2\n",
    "                        else:\n",
    "                            counter += corpus[j].count(v)\n",
    "                    top2.append(counter)\n",
    "                if all(x == 0 for x in top2):\n",
    "                    topicc.append('other')\n",
    "                else:\n",
    "                    highest_index = top2.index(max(top2))\n",
    "                    if max(top2) > 2:\n",
    "                        topicc.append(topic[highest_index])\n",
    "                    else:\n",
    "                        topicc.append('other')\n",
    "        df['topic'] = topicc\n",
    "        labelleddata = filename.replace(\".csv\", \"_labeled.csv\")\n",
    "        df.to_csv(labelleddata, index=False)\n",
    "        return df\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files(removalpath)\n",
    "\n",
    "def addkeywords (filename, dictionary, spam):\n",
    "    df = pd.read_csv(filename, encoding = \"latin2\")\n",
    "    df['topic'].unique()\n",
    "    topicss = list(df['topic'].unique())\n",
    "    counter = 0\n",
    "    for j in topicss:\n",
    "        counter += 1\n",
    "        print( )\n",
    "        print('... Processing topic', j, '...')\n",
    "        df1 = df[df.topic == j]\n",
    "        corpus = make_corpus(df1)\n",
    "        k = []\n",
    "        text = ''\n",
    "        for i in corpus:\n",
    "            i = i.encode('latin2','replace').lower()\n",
    "            k.append(i)\n",
    "        text = b''.join(k)\n",
    "        values = keywords(text=text,split='\\n',scores=True)\n",
    "        data = pd.DataFrame(values,columns=['keyword','score'])\n",
    "        data = data.sort_values('score',ascending=False)\n",
    "        key_word_list = data.head(40)\n",
    "        keywordlist = list(key_word_list['keyword'])\n",
    "        for i in keywordlist:\n",
    "            if i in spam:\n",
    "                keywordlist.remove(i)\n",
    "        for i in keywordlist:\n",
    "            if i in dictionary[j]:\n",
    "                keywordlist.remove(i)\n",
    "        dictionary[j].extend(keywordlist)\n",
    "        print('The recommended keywords for this topic are the following:', dictionary[j])\n",
    "\n",
    "def gettfidf(trainDF, d, count_vect, tfidf_vect):\n",
    "    print('... Transforming the validation data ...')\n",
    "    count = count_vect.transform(d)\n",
    "    tfidf_vect.fit(trainDF['corpus'])\n",
    "    tfidf = tfidf_vect.transform(d)\n",
    "    return tfidf   \n",
    "\n",
    "def gettfidf2(trainDF, d, count_vect, tfidf_vect):\n",
    "    print('... Transforming the training data ...')\n",
    "    count = count_vect.transform(d)\n",
    "    tfidf_vect.fit(trainDF['corpus'])\n",
    "    tfidf1 = tfidf_vect.transform(d)\n",
    "    return tfidf1\n",
    "\n",
    "def tdf (data):\n",
    "    labels, texts, ID = [], [], []\n",
    "    for i, line in data.iterrows():\n",
    "        labels.append(line['topic'])\n",
    "        texts.append(line['corpus'])\n",
    "        ID.append(line['ID'])\n",
    "    print('... Creating a dataframe using just texts and labels ...')\n",
    "    trainDF = pandas.DataFrame()\n",
    "    trainDF['corpus'] = texts\n",
    "    trainDF['topic'] = labels\n",
    "    trainDF['ID'] = ID\n",
    "    return trainDF        \n",
    "\n",
    "def trainsvm (file):\n",
    "    try:\n",
    "        removalpath = file.replace(\"/jk_sct_eng_complete_labeled.csv\", '/')\n",
    "        print( )\n",
    "        print('... Processing the training set ...')\n",
    "        data = pandas.read_csv(file, encoding = \"latin2\")\n",
    "        trainDF = tdf (data)\n",
    "        train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['corpus'], trainDF['topic'], \n",
    "                                                                              test_size=0.2, random_state=1993)\n",
    "        topicsinwords = list(trainDF['topic'].unique())\n",
    "        topicsinwords = sorted(topicsinwords)\n",
    "        print('... Encoding the topic number ...')\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        train_y = encoder.fit_transform(train_y)\n",
    "        valid_y = encoder.fit_transform(valid_y)\n",
    "        topicsinnumbers = list(set(list(train_y)))\n",
    "        count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', decode_error=\"replace\")\n",
    "        xtrain_count = count_vect.fit_transform(train_x)\n",
    "        picklepath = removalpath.replace('Python Notebooks and Scripts/emails/', '/')\n",
    "        picklepath1 = picklepath + \"feature.pkl\"\n",
    "        picklepath2 = picklepath + \"features.pkl\"\n",
    "        pickle.dump(count_vect.vocabulary_,open(picklepath1,\"wb\"))\n",
    "        xvalid_count = count_vect.transform(valid_x)\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000)\n",
    "        tfidf_vect.fit(trainDF['corpus'])\n",
    "        pickle.dump(tfidf_vect.vocabulary_,open(picklepath2,\"wb\"))\n",
    "        xtrain_tfidf = gettfidf(trainDF, train_x, count_vect, tfidf_vect)\n",
    "        xvalid_tfidf = gettfidf2(trainDF, valid_x, count_vect, tfidf_vect)\n",
    "        print('... Fitting the training dataset on the classifier ...')\n",
    "        Cs = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "        max_iter = [1, 10, 1000, 100, 10000]\n",
    "        param_grid = {'C': Cs, 'max_iter' : max_iter}\n",
    "        grid_search = GridSearchCV(svm.LinearSVC(), param_grid, cv = 3)\n",
    "        print('... Grid ...')\n",
    "        grid_search.fit(xtrain_tfidf, train_y)\n",
    "        C = grid_search.best_params_['C']\n",
    "        iterations = grid_search.best_params_['max_iter']\n",
    "        print('C =', C, ', Iterations =', iterations)\n",
    "        model = LinearSVC(C=C, max_iter = iterations)\n",
    "        model.fit(xtrain_tfidf, train_y)\n",
    "        valid_predictions_SVM = model.predict(xvalid_tfidf)\n",
    "        print(\"SVM Accuracy Score -> \",accuracy_score(valid_predictions_SVM, valid_y)*100)\n",
    "        check = pandas.DataFrame()\n",
    "        check['pred'] = valid_predictions_SVM\n",
    "        check['actual'] = valid_y\n",
    "        check['Index'] = valid_x.index\n",
    "        check.index = valid_x.index\n",
    "        p = list(check.index)\n",
    "        rowData = data.iloc[p, : ]\n",
    "        rowData.index = check.index\n",
    "        rowData['pred'] = check['pred']\n",
    "        rowData['pred'] = encoder.inverse_transform(rowData['pred'])\n",
    "        print(rowData[['subject', 'topic', 'pred']].head(30))\n",
    "        print(\"... Saving the tfidf dataframe ...\")\n",
    "        tfidfdf = pandas.DataFrame(xtrain_tfidf.toarray())\n",
    "        tfidfdf['label'] = train_y\n",
    "        tdfPath = picklepath + 'tfidf.csv'\n",
    "        labelingPath = picklepath + 'labeling.csv'\n",
    "        tfidfdf.to_csv(tdfPath, encoding = \"latin2\")\n",
    "        labelencoding = pandas.DataFrame()\n",
    "        labelencoding['topic_number'] = topicsinnumbers\n",
    "        labelencoding['topic_name'] = topicsinwords\n",
    "        labelencoding.to_csv(labelingPath, encoding = \"latin2\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files(removalpath)\n",
    "\n",
    "def calculate_wcss(data):\n",
    "    wcss = []\n",
    "    for n in range(2, 21):\n",
    "        kmeans = KMeans(n_clusters=n)\n",
    "        kmeans.fit(X=data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    return wcss\n",
    "\n",
    "def optimal_number_of_clusters(wcss):\n",
    "    x1, y1 = 2, wcss[0]\n",
    "    x2, y2 = 20, wcss[len(wcss)-1]\n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i+2\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "        denominator = sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "        distances.append(numerator/denominator)\n",
    "    return distances.index(max(distances)) + 2\n",
    "        \n",
    "def testsvm (output_file):\n",
    "    try:\n",
    "        removalpath = output_file.replace(\"/jk_sct_test.csv\", '/')\n",
    "        print( )\n",
    "        print('... Initiating testing ...')\n",
    "        cleanmails (output_file, 'en')\n",
    "        complete = output_file.replace('/jk_sct_test.csv', '/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv')\n",
    "        labelling = output_file.replace('/jk_sct_test.csv', '/labeling.csv')        \n",
    "        df7 = pandas.read_csv(complete, encoding = \"latin2\")\n",
    "        df8 = pandas.read_csv(labelling, encoding = \"latin2\")\n",
    "        tf = output_file.replace('/jk_sct_test.csv', '/tfidf.csv')  \n",
    "        picklepath1 = output_file.replace('/jk_sct_test.csv', '/feature.pkl')  \n",
    "        picklepath2 = output_file.replace('/jk_sct_test.csv', '/features.pkl') \n",
    "        tfidfdf = pandas.read_csv(tf, encoding = \"latin2\")\n",
    "        tfidfdf = tfidfdf.dropna()\n",
    "        train_y = tfidfdf['label']\n",
    "        xtrain_tfidf = tfidfdf.drop(['label'], axis=1)\n",
    "        xtrain_tfidf = xtrain_tfidf.drop(['0'], axis=1)\n",
    "        xtrain_tfidf = scipy.sparse.csr_matrix(xtrain_tfidf)\n",
    "        labels, textsss, labelno, subject, ID = [], [], [], [], []\n",
    "        textsss = make_corpus(df7)\n",
    "        subject = make_subject(df7)\n",
    "        ID = make_ID(df7)\n",
    "        for i, line in df8.iterrows():\n",
    "            labels.append(line['topic_name'])\n",
    "        for i, line in df8.iterrows():\n",
    "            labelno.append(line['topic_number'])\n",
    "        testDF = pandas.DataFrame()\n",
    "        testDF['corpus'] = textsss\n",
    "        for n in range(0, len(textsss)):\n",
    "            if type(textsss[n]) == str:\n",
    "                allwords_stemmed = tokenize_and_stem(textsss[n])\n",
    "                totalvocab_stemmed.extend(allwords_stemmed)\n",
    "                allwords_tokenized = tokenize_only(textsss[n])\n",
    "                totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "        print('... Vectorizing ...')\n",
    "        tf1 = open(picklepath1, 'rb')\n",
    "        tf2 = open(picklepath2, 'rb')\n",
    "        count_vect = CountVectorizer(decode_error=\"replace\", max_features=1000, vocabulary=pickle.load(tf1))\n",
    "        xtest_count = count_vect.fit_transform(testDF['corpus'])\n",
    "        pickle.dump(count_vect.vocabulary_,open(picklepath1,\"wb\"))\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', decode_error=\"replace\",\n",
    "                max_features=1000, tokenizer=tokenize_and_stem, stop_words='english', ngram_range=(1,3),\n",
    "                use_idf=True, vocabulary=pickle.load(tf2))\n",
    "        xtest_tfidf = tfidf_vect.fit(testDF['corpus'])\n",
    "        xtest_tfidf = tfidf_vect.transform(testDF['corpus'])\n",
    "        pickle.dump(tfidf_vect.vocabulary_,open(picklepath2,\"wb\"))\n",
    "        print('This is the shape of your tfidf matrix: ', xtest_tfidf.shape, xtrain_tfidf.shape)\n",
    "        terms = tfidf_vect.get_feature_names()\n",
    "        dist = 1 - cosine_similarity(xtest_tfidf)\n",
    "        print()\n",
    "        sum_of_squares = calculate_wcss(xtest_tfidf)\n",
    "        print('... Calculating the recommended number of topics in the dataset ...')\n",
    "        num_clusters = optimal_number_of_clusters(sum_of_squares)\n",
    "        print('According to the calculation, you have ', str(num_clusters), ' topics.')\n",
    "        print('... Grouping documents into ', str(num_clusters), 'clusters... ')\n",
    "        if len(textsss) > 500:\n",
    "            print('... This might take a while, as ', (len(textsss)), 'documents are being read ... ')\n",
    "        km = KMeans(n_clusters=num_clusters, random_state=3)\n",
    "        km.fit(xtest_tfidf)\n",
    "        clusters = km.labels_.tolist()\n",
    "        joblib.dump(km,  'doc_cluster.pkl')\n",
    "        km = joblib.load('doc_cluster.pkl')\n",
    "        clusters = km.labels_.tolist()\n",
    "        files = { 'title': subject, 'cluster': clusters, 'ID': ID}\n",
    "        frame = pd.DataFrame(files, index = [clusters] , columns = ['title', 'cluster', 'ID'])\n",
    "        frame['cluster'].value_counts()\n",
    "        print(\"Top terms per cluster:\")\n",
    "        print()\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "        for i in range(num_clusters):\n",
    "            print()\n",
    "            print(\"Cluster %d words:\" % i, end='')\n",
    "            for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "                print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "            print()\n",
    "            print(\"Files belonging to Cluster %d:\" % i, end='')\n",
    "            for title in frame.ix[i]['title'].values.tolist():\n",
    "                print(' %s,' % title, end='')\n",
    "            print()\n",
    "        print()\n",
    "        Cs = [0.1, 1, 10, 100, 1000, 10000]\n",
    "        max_iter = [10, 100, 1000, 10000]\n",
    "        param_grid = {'C': Cs, 'max_iter' : max_iter}\n",
    "        grid_search = GridSearchCV(svm.LinearSVC(), param_grid, cv = 3)\n",
    "        print('... Grid ...')\n",
    "        grid_search.fit(xtrain_tfidf, train_y)\n",
    "        C = grid_search.best_params_['C']\n",
    "        iterations = grid_search.best_params_['max_iter']\n",
    "        print('C =', C, ', Iterations =', iterations)\n",
    "        print('... Support Vector Machine doing its magic ...')\n",
    "        model = LinearSVC(C=C, max_iter = iterations, class_weight = 'balanced')\n",
    "        model.fit(xtrain_tfidf, train_y)\n",
    "        test_predictions_SVM = model.predict(xtest_tfidf)\n",
    "        check = pandas.DataFrame()\n",
    "        check['pred'] = test_predictions_SVM\n",
    "        rowData = df7\n",
    "        rowData.index = check.index\n",
    "        rowData['pred'] = check['pred']\n",
    "        for i in range(len(labels)):\n",
    "            rowData['pred'] = rowData['pred'].replace({labelno[i]:labels[i]})\n",
    "        rowdataname = removalpath + 'tested-labeled.csv'\n",
    "        rowData.to_csv(rowdataname, index=False)\n",
    "        print('File saved!')\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files(removalpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "rubbish = [\"intended recipient\", \"designated recipient\", \"consider the environmental\", \"may contain confidential\", \n",
    "           \"may be confidential\", \"received this in error\", \"communication in error\", \"privileged and confidential\",\n",
    "           \"environmental impact\", \"consider the environment\",  \"before printing\", \"Before printing\", \"Twitter\",\n",
    "           \"from an EXTERNAL SENDER\", \"Sent: \", \"Cc: \", \"LinkedIn\", \"Sent from my iPhone\", \"Thank you in advance.\", \"RE: \",\n",
    "          'vriendelijke', 'Euregio Digital Delivery Center', 'information that is confidential', 'for the person or entity']\n",
    "\n",
    "\n",
    "def remove_files (folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "def removepersonalnames(text):\n",
    "    document_big = nlp_big(text)\n",
    "    entities_big = [e.string for e in document_big.ents if 'PERSON'==e.label_] \n",
    "    entities_big = list(entities_big) \n",
    "    for i in entities_big:\n",
    "        if i in text:\n",
    "            text = text.replace(i, '')\n",
    "    return text\n",
    "\n",
    "def cleanmails(data, lang):\n",
    "    try:\n",
    "        splitted = data.split('/')\n",
    "        output_folder = data.strip(splitted[-1])\n",
    "        removalpath = output_folder + 'Python Notebooks and Scripts/emails/'\n",
    "        print( )\n",
    "        print('... Initiating the cleaning process ...')\n",
    "        df = pandas.read_csv(data, header=None, encoding = \"latin2\")\n",
    "        sep1 = '< > > wrote:'\n",
    "        sep2 = ' > het volgende geschreven:'\n",
    "        oldreplacebody = ('\\r', '\\t','\\n \\n','\\n\\n\\n\\n','\\n\\n\\n','\\n\\n')\n",
    "        newreplacebody = ('','',\"\\n\",\"\\n\",\"\\n\",\"\\n\")\n",
    "        oldreplacesub = ('[External]', 'FW: ', 'Fw: ', 'Fwd: ', 'RE: ', 'Re: ')\n",
    "        newreplacesub = ('','','', '','', '')\n",
    "        for i,j in df.iterrows():\n",
    "            j[2] = str(j[2])\n",
    "            j[2] = re.sub('<[^>]+>', '< >', j[2], flags=re.MULTILINE)\n",
    "            j[2] = re.sub('To:[^>]+\\n', '', j[2], flags=re.MULTILINE)\n",
    "            j[2] = j[2].split(sep1, 1)[0]\n",
    "            j[2] = j[2].split(sep2, 1)[0]\n",
    "            for check, rep in zip(oldreplacebody, newreplacebody):\n",
    "                j[2] = j[2].replace(check, rep)\n",
    "            j[0] = str(j[0])    \n",
    "            for check, rep in zip(oldreplacesub, newreplacesub):\n",
    "                j[0] = j[0].replace(check, rep)\n",
    "        df.rename(columns={0:'subject', 1:'sender_address', 2: \"body\"}, inplace=True)\n",
    "        corpus, ID = [], []\n",
    "        global cellist\n",
    "        cellist = []\n",
    "        for i,j in df.iterrows():\n",
    "            ID.append(str(uuid.uuid4()))   \n",
    "        df['ID'] = ID\n",
    "        for i,j in df.iterrows():\n",
    "            corpus.append(j['body'])\n",
    "        length = len(df)\n",
    "        if len(df) == len(corpus):\n",
    "            print('The total number of files to clean is ', str(len(df)))\n",
    "        global nlp_big\n",
    "        nlp_big =  spacy.load('en_core_web_md')\n",
    "        if length > 10:\n",
    "            if length > 1000:\n",
    "                counter = round(length/50)\n",
    "            else:\n",
    "                counter = round(length/10)\n",
    "            for r in range(0, len(corpus)):\n",
    "    #            corpus[r] = removepersonalnames(corpus[r])\n",
    "                convert(corpus[r], output_folder)\n",
    "                perc = r * 100/length\n",
    "                rounded = \"{0:.2f}\".format(perc)\n",
    "                if r % counter == 0:\n",
    "                    print('... Approximately', rounded, ' % of your files are clean, currently cleaning file no. ', len(cellist), '...')\n",
    "        else:\n",
    "                counter = length\n",
    "                for r in range(0, len(corpus)):\n",
    "    #                corpus[r] = removepersonalnames(corpus[r])\n",
    "                    convert(corpus[r], output_folder)\n",
    "        if output_folder[-1] != \"/\":\n",
    "            distilled_read = output_folder + '/Python Notebooks and Scripts/emails/jk2.csv'\n",
    "        else:\n",
    "            distilled_read = output_folder + 'Python Notebooks and Scripts/emails/jk2.csv'\n",
    "        df3 = pandas.read_csv(distilled_read, header=None, encoding = \"latin2\")\n",
    "        print('... Filtering emails in English ...')\n",
    "        df.index = df3.index\n",
    "        df3['sender_address'] = df['sender_address']\n",
    "        df3['subject'] = df['subject']\n",
    "        df3['body'] = df['body']\n",
    "        df3['ID'] = df['ID']\n",
    "        df3.rename(columns={0:'corpus'}, inplace=True)\n",
    "        df3['corpus'] = df3['corpus'].map(str) + '\\n' + df3['subject'] + \" \" + df3['subject'] + \" \" + df3['subject']\n",
    "        lang1 = []\n",
    "        for i,j in df3.iterrows():\n",
    "            j['corpus'] = str(j['corpus'])\n",
    "            if len(j['corpus']) > 100:\n",
    "                l = detect(j['corpus'])\n",
    "            else:\n",
    "                l = \"email too short\"\n",
    "            lang1.append(l)\n",
    "        df3['language'] = lang1\n",
    "        df4 = df3.loc[df3.language == str(lang)]\n",
    "        if output_folder[-1] != \"/\":\n",
    "            language_filtered = output_folder + '/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv'\n",
    "        else:\n",
    "            language_filtered = output_folder + 'Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv'\n",
    "        df4.to_csv(language_filtered, index=False)\n",
    "        print ()\n",
    "        print ('CLEANING COMPLETE')\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files(removalpath)\n",
    "\n",
    "def convert(text, output_folder, threshold=.95):\n",
    "    pos_tagger = English()\n",
    "    original_email = text\n",
    "    sentences = _corpus2sentences(original_email)\n",
    "    _generate_text(sentences, output_folder)\n",
    "\n",
    "def _corpus2sentences(text):\n",
    "    return text.strip().split('\\n')\n",
    "\n",
    "def _generate_text(sentences, output_folder, threshold=0.95):\n",
    "    sen = '\\n'\n",
    "    clean = \"\"\n",
    "    for sentence in sentences: \n",
    "        if _prob_block(sentence, nlp_big) < threshold:\n",
    "            if not any(r in sentence for r in rubbish):\n",
    "                if clean == '':\n",
    "                    clean = sen + sentence\n",
    "                else:\n",
    "                    clean = clean + sen + sentence\n",
    "    _generate_list(clean, output_folder)\n",
    "\n",
    "def _generate_list(clean, output_folder):\n",
    "    cellist.append(clean)\n",
    "    if output_folder[-1] != \"/\":\n",
    "        distilled = output_folder + '/Python Notebooks and Scripts/emails/jk2.csv'\n",
    "    else:\n",
    "        distilled = output_folder + 'Python Notebooks and Scripts/emails/jk2.csv'\n",
    "    with open(distilled, 'wb') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for h in cellist:\n",
    "            wr.writerow([h])            \n",
    "            \n",
    "def _prob_block(sentence, pos_tagger):   \n",
    "    doc = pos_tagger(sentence)\n",
    "    verb_count = np.sum([token.pos_ != \"VERB\" for token in doc])\n",
    "    length = float(len(doc))\n",
    "    thing = 0\n",
    "    if verb_count > 0:\n",
    "        thing = verb_count/length\n",
    "    else:\n",
    "        thing == 0.0\n",
    "    return thing\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def make_corpus(dataframe):\n",
    "    corpus = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        corpus.append(j['corpus'])\n",
    "    return corpus\n",
    "\n",
    "def make_subject(dataframe):\n",
    "    subject = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        subject.append(j['subject'])\n",
    "    return subject\n",
    "\n",
    "def make_ID(dataframe):\n",
    "    ID = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        ID.append(j['ID'])\n",
    "    return ID\n",
    "\n",
    "def unique(element, listt):\n",
    "    newlist = []\n",
    "    for l in listt:\n",
    "        newlist.append(l)\n",
    "    return listt.count(element) == 1\n",
    "\n",
    "def gettfidf(trainDF, d, count_vect, tfidf_vect):\n",
    "    print('... Transforming the validation data ...')\n",
    "    count = count_vect.transform(d)\n",
    "    tfidf_vect.fit(trainDF['corpus'])\n",
    "    tfidf = tfidf_vect.transform(d)\n",
    "    return tfidf   \n",
    "\n",
    "def gettfidf2(trainDF, d, count_vect, tfidf_vect):\n",
    "    print('... Transforming the training data ...')\n",
    "    count = count_vect.transform(d)\n",
    "    tfidf_vect.fit(trainDF['corpus'])\n",
    "    tfidf1 = tfidf_vect.transform(d)\n",
    "    return tfidf1\n",
    "\n",
    "def tdf (data):\n",
    "    labels, texts, ID = [], [], []\n",
    "    for i, line in data.iterrows():\n",
    "        labels.append(line['topic'])\n",
    "        texts.append(line['corpus'])\n",
    "        ID.append(line['ID'])\n",
    "    print('... Creating a dataframe using just texts and labels ...')\n",
    "    trainDF = pandas.DataFrame()\n",
    "    trainDF['corpus'] = texts\n",
    "    trainDF['topic'] = labels\n",
    "    trainDF['ID'] = ID\n",
    "    return trainDF\n",
    "\n",
    "def testsvmm (output_folder):\n",
    "    try:\n",
    "        #removalpath = output_folder.replace(\"/jk_sct_test1.csv\", '/')\n",
    "        output_file = ''\n",
    "        if output_folder[-1] != \"/\":\n",
    "            output_file = output_folder + '/jk_sct_test1.csv'\n",
    "            removalpath = output_folder + '/'\n",
    "        else:\n",
    "            output_file = output_folder + 'jk_sct_test1.csv'\n",
    "            removalpath = output_folder\n",
    "        print('... Classification initiated ...')\n",
    "        cleanmails (output_file, 'en')\n",
    "        complete = output_file.replace('/jk_sct_test1.csv', '/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv')\n",
    "        labelling = output_file.replace('/jk_sct_test1.csv', '/labeling.csv')        \n",
    "        df7 = pandas.read_csv(complete, encoding = \"latin2\")\n",
    "        df8 = pandas.read_csv(labelling, encoding = \"latin2\")\n",
    "        tf = output_file.replace('/jk_sct_test1.csv', '/tfidf.csv')  \n",
    "        picklepath1 = output_file.replace('/jk_sct_test1.csv', '/feature.pkl')  \n",
    "        picklepath2 = output_file.replace('/jk_sct_test1.csv', '/features.pkl') \n",
    "        tfidfdf = pandas.read_csv(tf, encoding = \"latin2\")        \n",
    "        tfidfdf = tfidfdf.dropna()\n",
    "        train_y = tfidfdf['label']\n",
    "        xtrain_tfidf = tfidfdf.drop(['label'], axis=1)\n",
    "        xtrain_tfidf = xtrain_tfidf.drop(['0'], axis=1)\n",
    "        xtrain_tfidf = scipy.sparse.csr_matrix(xtrain_tfidf)\n",
    "        labels, textsss, labelno, subject, ID = [], [], [], [], []\n",
    "        textsss = make_corpus(df7)\n",
    "        subject = make_subject(df7)\n",
    "        ID = make_ID(df7)\n",
    "        for i, line in df8.iterrows():\n",
    "            labels.append(line['topic_name'])\n",
    "        for i, line in df8.iterrows():\n",
    "            labelno.append(line['topic_number'])\n",
    "        testDF = pandas.DataFrame()\n",
    "        testDF['corpus'] = textsss\n",
    "        for n in range(0, len(textsss)):\n",
    "            if type(textsss[n]) == str:\n",
    "                allwords_stemmed = tokenize_and_stem(textsss[n])\n",
    "                totalvocab_stemmed.extend(allwords_stemmed)\n",
    "                allwords_tokenized = tokenize_only(textsss[n])\n",
    "                totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "        print('... Vectorizing ...')\n",
    "        tf1 = open(picklepath1, 'rb')\n",
    "        tf2 = open(picklepath2, 'rb')\n",
    "        count_vect = CountVectorizer(decode_error=\"replace\", max_features=1000, vocabulary=pickle.load(tf1))\n",
    "        xtest_count = count_vect.fit_transform(testDF['corpus'])\n",
    "        pickle.dump(count_vect.vocabulary_,open(picklepath1,\"wb\"))\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', decode_error=\"replace\",\n",
    "                max_features=1000, tokenizer=tokenize_and_stem, stop_words='english', ngram_range=(1,3),\n",
    "                use_idf=True, vocabulary=pickle.load(tf2))\n",
    "        xtest_tfidf = tfidf_vect.fit(testDF['corpus'])\n",
    "        xtest_tfidf = tfidf_vect.transform(testDF['corpus'])\n",
    "        pickle.dump(tfidf_vect.vocabulary_,open(picklepath2,\"wb\"))\n",
    "        print('This is the shape of your tfidf matrix: ', xtest_tfidf.shape, xtrain_tfidf.shape)\n",
    "        Cs = [0.1, 1, 10, 100, 1000, 10000]\n",
    "        max_iter = [10, 100, 1000, 10000]\n",
    "        param_grid = {'C': Cs, 'max_iter' : max_iter}\n",
    "        grid_search = GridSearchCV(svm.LinearSVC(), param_grid, cv = 3)\n",
    "        print('... Grid ...')\n",
    "        grid_search.fit(xtrain_tfidf, train_y)\n",
    "        C = grid_search.best_params_['C']\n",
    "        iterations = grid_search.best_params_['max_iter']\n",
    "        print('C =', C, ', Iterations =', iterations)\n",
    "        print('... Support Vector Machine doing its magic ...')\n",
    "        model = LinearSVC(C=C, max_iter = iterations, class_weight = 'balanced')\n",
    "        model.fit(xtrain_tfidf, train_y)\n",
    "        test_predictions_SVM = model.predict(xtest_tfidf)\n",
    "        check = pandas.DataFrame()\n",
    "        check['pred'] = test_predictions_SVM\n",
    "        rowData = df7\n",
    "        rowData.index = check.index\n",
    "        rowData['pred'] = check['pred']\n",
    "        for i in range(len(labels)):\n",
    "            rowData['pred'] = rowData['pred'].replace({labelno[i]:labels[i]})\n",
    "        rowdataname = removalpath + 'tested-labeled.csv'\n",
    "        rowData.to_csv(rowdataname, index=False)\n",
    "        print('File saved!')\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files(removalpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import_train_test('C:/Users/andrea.feher/Downloads/jk_sct.csv', 'support_robotics@accenture.com', \"Postvak IN\", \n",
    "                  2018, 9, 1, 0, 0, 0, \n",
    "           {'urgent' : ['p1', 'urgent', 'important', 'asap', 'high priority', 'priority','p2', \n",
    "                       'as soon as possible', 'stopped', 'priority 1', 'prompt', 'urgent:', 'important:'],\n",
    "           'incident/change tickets' : ['description: ', 'assigned to', 'automatically generated', 'incident', \n",
    "                        'change', 'assigned', 'on your behalf', 'click here'], \n",
    "           'credentials' : ['password', 'new', 'reset', 'maintenance', 'credential', 'login', 'log in', \n",
    "                        'credentials', 'expire', 'user'], \n",
    "            'pop-up' : ['pop-up', 'popup', 'pop up', 'popping', 'popped', 'block', 'message', 'window'], \n",
    "             'infrastructure' : ['connection', 'sap', 'connectivity', 'virtual machine', 'item', 'reboot', \n",
    "                        'output', 'inform', 'fail', 'failing', 'infrastructure'], \n",
    "           'meetings' : ['call', 'meet', 'event', 'meeting', 'meetings', 'call', 'calls', 'join', 'joining', \n",
    "                         'skype', 'teams', 'schedule', 'tomorrow'], \n",
    "           'reports' : ['report', 'daily', 'snow', 'performance', 'process', 'service-now', 'complete', \n",
    "                        'reports', 'servicenow', 'service now', 'status'],\n",
    "           'SLA_warning' : ['sla', 'warning', 'breach', 'task state', 'robotics', 'task'], \n",
    "           'documentation' : ['pdd', 'tdd', 'sdd', 'share','documentation', 'fdd', 'change', 'file', 'manual', \n",
    "                        'brd',  'update', 'review', 'design document'], \n",
    "           'lifecycle' : ['handover', 'lifecycle', 'go-live', 'go live', 'test', 'testing', 'knowledge transfer', \n",
    "                          'hand-over'],\n",
    "           'other' : ['out of office','office']\n",
    "          }, ['robotics', 'issue', 'run', 'runs', 'flight', 'flights', 'running', 'case', 'case', 'robots', 'errors', \n",
    "         'rpa', 'nice', 'process', 'processing', 'processes', 'processed', 'email', 'emails', 'thank', 'thanks'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_test( {'urgent' : ['p1', 'urgent', 'important', 'asap', 'high priority', 'priority','p2', \n",
    "                       'as soon as possible', 'stopped', 'priority 1', 'prompt', 'urgent:', 'important:'],\n",
    "           'incident/change tickets' : ['description: ', 'assigned to', 'automatically generated', 'incident', \n",
    "                        'change', 'assigned', 'on your behalf', 'click here'], \n",
    "           'credentials' : ['password', 'new', 'reset', 'maintenance', 'credential', 'login', 'log in', \n",
    "                        'credentials', 'expire', 'user'], \n",
    "            'pop-up' : ['pop-up', 'popup', 'pop up', 'popping', 'popped', 'block', 'message', 'window'], \n",
    "             'infrastructure' : ['connection', 'sap', 'connectivity', 'virtual machine', 'item', 'reboot', \n",
    "                        'output', 'inform', 'fail', 'failing', 'infrastructure'], \n",
    "           'meetings' : ['call', 'meet', 'event', 'meeting', 'meetings', 'call', 'calls', 'join', 'joining', \n",
    "                         'skype', 'teams', 'schedule', 'tomorrow'], \n",
    "           'reports' : ['report', 'daily', 'snow', 'performance', 'process', 'service-now', 'complete', \n",
    "                        'reports', 'servicenow', 'service now', 'status'],\n",
    "           'SLA_warning' : ['sla', 'warning', 'breach', 'task state', 'robotics', 'task'], \n",
    "           'documentation' : ['pdd', 'tdd', 'sdd', 'share','documentation', 'fdd', 'change', 'file', 'manual', \n",
    "                        'brd',  'update', 'review', 'design document'], \n",
    "           'lifecycle' : ['handover', 'lifecycle', 'go-live', 'go live', 'test', 'testing', 'knowledge transfer', \n",
    "                          'hand-over'],\n",
    "           'other' : ['out of office','office']\n",
    "          }, ['robotics', 'issue', 'run', 'runs', 'flight', 'flights', 'running', 'case', 'case', 'robots', 'errors', \n",
    "         'rpa', 'nice', 'process', 'processing', 'processes', 'processed', 'email', 'emails', 'thank', 'thanks'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test({'kahoots' : ['kahoot', 'quiz', 'answers', 'questions', 'correct', 'account'], \n",
    "    'courses/learning boards' : ['learning board', 'course', 'training', 'exam', 'program', 'register', 'skill', 'completed', 'transcript', 'curriculum', 'accreditation'],  \n",
    "    'meetings' : ['meeting', 'canceled', 'event', 'cancelled', 'meeting', 'attend', 'rpa team', 'lunch', 'join', 'session', 'party'], \n",
    "    'myTE' : ['myte', 'wbs', 'roll', 'myt&e', 'submit', 'submission', 'hours', 'unassigned', 'chargeable'], \n",
    "    'messages' : ['skype', 'microsoft teams', 'message', 'join', 'conversation', 'video'],\n",
    "    'accenture spam': ['newsletter', 'action', 'gallia', 'feedback', 'take action', 'habit', 'conversant', 'trending', 'award', 'versed', 'tech'],\n",
    "    'other' : [\"password\"]},\n",
    "    ['email', 'emails', 'accenture', 'thank', 'thanks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsvmm('C:/Users/andrea.feher/Downloads/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing mailbox:ANDREA.FEHER@ACCENTURE.COM\n",
      "\n",
      "... Initiating the cleaning process ...\n",
      "The total number of files to clean is  691\n",
      "... Approximately 0.00  % of your files are clean, currently cleaning file no.  1 ...\n",
      "... Approximately 9.99  % of your files are clean, currently cleaning file no.  70 ...\n",
      "... Approximately 19.97  % of your files are clean, currently cleaning file no.  139 ...\n",
      "... Approximately 29.96  % of your files are clean, currently cleaning file no.  208 ...\n",
      "... Approximately 39.94  % of your files are clean, currently cleaning file no.  277 ...\n",
      "... Approximately 49.93  % of your files are clean, currently cleaning file no.  346 ...\n",
      "... Approximately 59.91  % of your files are clean, currently cleaning file no.  415 ...\n",
      "... Approximately 69.90  % of your files are clean, currently cleaning file no.  484 ...\n",
      "... Approximately 79.88  % of your files are clean, currently cleaning file no.  553 ...\n",
      "... Approximately 89.87  % of your files are clean, currently cleaning file no.  622 ...\n",
      "... Approximately 99.86  % of your files are clean, currently cleaning file no.  691 ...\n",
      "... Filtering emails in English ...\n",
      "\n",
      "CLEANING COMPLETE\n",
      "\n",
      "... Clustering into your 10 topics according to the keywords given ...\n",
      "\n",
      "... Processing topic meetings ...\n",
      "The recommended keywords for this topic are the following: ['meeting', 'event', 'attend', 'rpa team', 'lunch', 'join', 'session', 'party', 'accentures', 'innovator', 'innovators', 'innovating', 'innovate', 'innovative', 'innovations', 'new', 'news', 'events', 'teams', 'joining', 'joined', 'meet', 'meetings', 'rpa team', 'work', 'works', 'industries', 'client', 'clients', 'thanking', 'innovation dont miss', 'van', 'diversity join', 'time', 'times', 'heerlen', 'business', 'businesses', 'sessions', 'management', 'manager', 'managers', 'manage']\n",
      "\n",
      "... Processing topic other ...\n",
      "The recommended keywords for this topic are the following: ['password', 'new', 'news', 'accentures', 'thank', 'thanking', 'service', 'services', 'emails', 'requested', 'request', 'requests', 'accenture resources', 'managed', 'management', 'manage', 'python', 'updates', 'updating', 'updated', 'update', 'mail', 'let', 'lets', 'day', 'days', 'github', 'addressed', 'work', 'working', 'works', 'worked', 'process', 'processes', 'click', 'clicking', 'learning', 'recycle', 'recycled']\n",
      "\n",
      "... Processing topic myTE ...\n",
      "The recommended keywords for this topic are the following: ['myte', 'wbs', 'roll', 'myt&e', 'submit', 'submission', 'hours', 'unassigned', 'chargeable', 'times', 'approval', 'approved', 'approver', 'approve', 'rolled', 'myte information', 'thank', 'time report', 'required', 'requires', 'require', 'requirements', 'andrea', 'project', 'lead', 'requests', 'requested', 'requester', 'requesting', 'informed', 'action', 'actions', 'request access', 'management', 'managed', 'managing', 'dont', 'accessed', 'notification', 'contact', 'contacts', 'van', 'accentures', 'technology support', 'period']\n",
      "\n",
      "... Processing topic accenture spam ...\n",
      "The recommended keywords for this topic are the following: ['action', 'newsletter', 'gallia', 'feedback', 'take action', 'habit', 'conversant', 'trending', 'award', 'versed', 'tech', 'technologys', 'technologies', 'technological', 'news', 'new', 'accelerate accentures', 'innovator', 'innovating', 'innovations', 'innovation', 'innovate', 'innovators', 'services', 'service', 'business', 'businesses', 'busy', 'right technology', 'toms innovative', 'teamed', 'teams', 'team', 'read', 'reading', 'manager', 'manage', 'managers', 'managed', 'manages', 'clients', 'companies', 'companys', 'company', 'actionable', 'actions', 'learn', 'learning', 'year']\n",
      "\n",
      "... Processing topic courses/learning boards ...\n",
      "The recommended keywords for this topic are the following: ['learning board', 'course', 'training', 'exam', 'program', 'register', 'skill', 'completed', 'transcript', 'curriculum', 'accreditation', 'train', 'trainings', 'training additional', 'accentures', 'initially trained', 'learn', 'learned', 'new', 'news', 'secure', 'contact learning', 'complete', 'completing', 'completions', 'timely completion', 'time', 'timing', 'times', 'informed', 'inform', 'programs', 'programming', 'heerlen', 'following', 'follow', 'follows', 'right', 'rights', 'direct', 'direction', 'directions', 'cancellation', 'cancel', 'cancellations', 'cancelled']\n",
      "\n",
      "... Processing topic messages ...\n",
      "The recommended keywords for this topic are the following: ['skype', 'microsoft teams', 'video', 'join', 'conversation', 'message', 'team', 'new', 'news', 'accentures', 'meetings', 'meeting', 'joined', 'joining', 'support', 'supportive', 'messages', 'business', 'busy', 'innovators', 'innovation', 'innovative', 'videos', 'people', 'mobile', 'time', 'like', 'learn', 'learned', 'haha', 'growth performance', 'services', 'experience', 'experiment', 'video calls', 'communicating', 'communicate', 'communications', 'communication', 'community', 'workshop']\n",
      "\n",
      "... Processing topic veon ...\n",
      "The recommended keywords for this topic are the following: ['chatbot', 'vica', 'veon', 'access', 'azure', 'account', 'documentation', 'code', 'veons', 'documents', 'subject veon', 'updated document', 'latest documentation', 'accounts', 'user', 'users', 'tomorrow', 'rachelle', 'currently', 'current', 'center', 'dont', 'thank', 'today', 'joining', 'join', 'meeting', 'information', 'inform', 'informed', 'follow', 'following', 'chatbots', 'andrea', 'environment', 'functional', 'functions', 'function', 'functionality', 'functionalities', 'review', 'reviewing', 'plan', 'planned']\n",
      "\n",
      "... Processing topic atr ...\n",
      "The recommended keywords for this topic are the following: ['nlp', 'atr', 'dispatcher', 'malta', 'robot', 'parts', 'spares', 'quotation', 'estimate', 'nlp requirements', 'mail', 'require', 'requires', 'mailbox', 'project', 'projects', 'arp', 'sending mails', 'clients', 'team', 'teams', 'current', 'email', 'activities', 'active', 'knowledge', 'understanding', 'understand', 'let', 'lets', 'content thanks', 'security currently', 'technical', 'start', 'starting', 'started', 'client visit', 'week', 'weeks', 'released', 'release', 'processes', 'process', 'session', 'sessions']\n",
      "\n",
      "... Processing topic travelling ...\n",
      "The recommended keywords for this topic are the following: ['passenger', 'boarding', 'travel', 'hotel', 'ticket', 'booking', 'terminal', 'flight', 'travelling', 'travelers', 'traveler', 'innovation', 'emails', 'trip', 'trips', 'bookings', 'booked', 'book', 'scale innovations', 'accentures', 'oct', 'companies', 'confirm', 'confirmation', 'confirmed', 'services', 'information', 'informed', 'checking', 'check', 'program', 'programs', 'online', 'company uses', 'flights', 'clients', 'client', 'home', 'mobile', 'international business', 'insurance', 'cwt service', 'british airways']\n",
      "\n",
      "... Processing topic kahoots ...\n",
      "The recommended keywords for this topic are the following: ['kahoot', 'quiz', 'answers', 'questions', 'correct', 'account', 'answers', 'answer', 'question', 'kahoots tab', 'trainings', 'training', 'support portal', 'following', 'follow', 'users', 'help', 'helps', 'accounts', 'existing user', 'deadlines', 'deadline', 'plan', 'access', 'created', 'create', 'document', 'documents', 'attached', 'day', 'days', 'busy', 'case', 'cases', 'information', 'started', 'time', 'weeks', 'week', 'rpa business', 'new', 'sessions']\n",
      "\n",
      "... Processing the training set ...\n",
      "... Creating a dataframe using just texts and labels ...\n",
      "... Encoding the topic number ...\n",
      "... Transforming the validation data ...\n",
      "... Transforming the training data ...\n",
      "... Fitting the training dataset on the classifier ...\n",
      "... Grid ...\n",
      "C = 1 , Iterations = 10\n",
      "SVM Accuracy Score ->  87.09677419354838\n",
      "                                               subject  \\\n",
      "596  Reminder: Join the Ready, Set, Accenture Broad...   \n",
      "57              MAILS SPARES ORDERS AND QUOTATIONS - 2   \n",
      "580                      Caveman Run 2019 - Cancelled!   \n",
      "407                                         Activities   \n",
      "601                                 WBS - Andrea Hours   \n",
      "163  Reward season is here for AI Conversant comple...   \n",
      "615   Training availability Netherlands (October 21...   \n",
      "398                        IQ Bot training in Heerlen!   \n",
      "27                Update: Employee Satisfaction Survey   \n",
      "217  FINAL: REMINDER: Ellie Lust talks about Inclus...   \n",
      "85                 New Accreditation Heerlen: Cogito!    \n",
      "605  inline code documentation + basic branch descr...   \n",
      "577                New Accreditation Heerlen: Cogito!    \n",
      "138  AI Versed Virtual Learning Series: AI Industry...   \n",
      "522  Automatic reply: PLACEHOLDER: Business Update ...   \n",
      "114  inline code documentation + basic branch descr...   \n",
      "284                                   Kahoot questions   \n",
      "573                    AI Capability AISC - Next steps   \n",
      "436                               Roll On Confirmation   \n",
      "418   You have completed COD 101 - Fundamentals of ...   \n",
      "49    You have completed AWA 101 - Fundamentals of ...   \n",
      "226  Your new Letter request LTR0009008 has been cr...   \n",
      "34   Announcing changes to the Avanade Gallia Leade...   \n",
      "454    Install essential drivers updates when prompted   \n",
      "73                                         RPA Meeting   \n",
      "437            ATR: Dispatching Procedure/Requirements   \n",
      "516      Automated RPA Service Desk - L1 Notifications   \n",
      "289         Video ATC RPA COE network - HFS Roundtable   \n",
      "558  AI Versed Virtual Learning Series: AI Industry...   \n",
      "421                                  Team Event - Menu   \n",
      "\n",
      "                       topic                     pred  \n",
      "596                 meetings                 meetings  \n",
      "57                       atr                      atr  \n",
      "580                 meetings                 meetings  \n",
      "407                      atr                      atr  \n",
      "601                     myTE                     myTE  \n",
      "163           accenture spam           accenture spam  \n",
      "615  courses/learning boards  courses/learning boards  \n",
      "398  courses/learning boards  courses/learning boards  \n",
      "27            accenture spam                    other  \n",
      "217                 meetings                 meetings  \n",
      "85   courses/learning boards  courses/learning boards  \n",
      "605                     veon                 meetings  \n",
      "577  courses/learning boards  courses/learning boards  \n",
      "138                 meetings                 meetings  \n",
      "522                    other                    other  \n",
      "114                     veon                     veon  \n",
      "284                  kahoots                  kahoots  \n",
      "573                      atr                      atr  \n",
      "436                     myTE                     myTE  \n",
      "418  courses/learning boards  courses/learning boards  \n",
      "49   courses/learning boards  courses/learning boards  \n",
      "226                    other                    other  \n",
      "34            accenture spam           accenture spam  \n",
      "454                    other                    other  \n",
      "73                  meetings                 meetings  \n",
      "437                      atr                      atr  \n",
      "516                    other                    other  \n",
      "289                 messages                 meetings  \n",
      "558                 meetings                 meetings  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421                 meetings                 meetings  \n",
      "... Saving the tfidf dataframe ...\n",
      "\n",
      "... Initiating testing ...\n",
      "\n",
      "... Initiating the cleaning process ...\n",
      "The total number of files to clean is  293\n",
      "... Approximately 0.00  % of your files are clean, currently cleaning file no.  1 ...\n",
      "... Approximately 9.90  % of your files are clean, currently cleaning file no.  30 ...\n",
      "... Approximately 19.80  % of your files are clean, currently cleaning file no.  59 ...\n",
      "... Approximately 29.69  % of your files are clean, currently cleaning file no.  88 ...\n",
      "... Approximately 39.59  % of your files are clean, currently cleaning file no.  117 ...\n",
      "... Approximately 49.49  % of your files are clean, currently cleaning file no.  146 ...\n",
      "... Approximately 59.39  % of your files are clean, currently cleaning file no.  175 ...\n",
      "... Approximately 69.28  % of your files are clean, currently cleaning file no.  204 ...\n",
      "... Approximately 79.18  % of your files are clean, currently cleaning file no.  233 ...\n",
      "... Approximately 89.08  % of your files are clean, currently cleaning file no.  262 ...\n",
      "... Approximately 98.98  % of your files are clean, currently cleaning file no.  291 ...\n",
      "... Filtering emails in English ...\n",
      "\n",
      "CLEANING COMPLETE\n",
      "... Vectorizing ...\n",
      "This is the shape of your tfidf matrix:  (260, 1000) (495, 1000)\n",
      "\n",
      "... Calculating the recommended number of topics in the dataset ...\n",
      "According to the calculation, you have  13  topics.\n",
      "... Grouping documents into  13 clusters... \n",
      "Top terms per cluster:\n",
      "\n",
      "\n",
      "Cluster 0 words: b'ai', b'learn', b'platform', b'session', b'bots', b'attendants',\n",
      "Files belonging to Cluster 0: RECORDING NOW AVAILABLE: AI Deep Dive Webinar Series - Accenture Robotics Platform 2019.05 Release  July 12, 2019, Thank you for Participating in the FY19 AI Conversant Campaign, AI Case Studies Harvesting Campaign launch, Launch of Voice AI Learning, AI Versed Virtual Learning Series: AI Industry Applications for NA/Europe/LATAM on August 8, Julie Sweet appointed new CEO of Accenture, LOGISTICS : AI Deep Dive Webinar Series - Accenture Robotics Platform 2019.05 Release  July 12, 2019, SAVE THE DATE: AI Deep Dive Webinar Series - Accenture Robotics Platform 2019.05 Release  July 12, 2019,\n",
      "\n",
      "Cluster 1 words: b'event', b'team', b'responding', b'send', b'maastricht', b'soon',\n",
      "Files belonging to Cluster 1: Team Event Questionnaire, IASC - Team Event!!, Event, Staffing made easier, Canceled: Placeholder - Team Event, Placeholder: IA Team Event - Bites & Drinks in Maastricht - Christmas Edition, IA/AI Team Event - Ready to Celebrate?!, Team Event Questionnaire GDPR, REMINDER - Team Event Placeholders,\n",
      "\n",
      "Cluster 2 words: b'new', b'make', b'tech', b'feedback', b'work', b'fy20',\n",
      "Files belonging to Cluster 2: Campus Newsletter SSC nr. 146, July 2019, Your latest downloads - share your feedback,  Rimo, Innovation Week: Are You #ALLIn?,  Reminder of Pearson VUE Exam Appointment, Technology NL  I&D - New leads and open positions, Stay Future Ready - Learn Python!, Official holidays FY20 and mandatory day off, Technology Demand wk 24,  You have completed AWA 102 - Secure Software Concepts, New joiners Intelligent Automation - September 2019, AA - Training Course, Community: New Joiner Day 2 Heerlen (FY19), FY19 Information Security Awareness Week - Gallia ATC June 2019, HONESTY in feedback: Make it a habit, make it TECH!, Career Counselor Role and Expectations, Hit the road, hackers. New Hacker Land video!, The futures spec-tacular, UiPath APG - Dashboard, New Joiners Postcard NL June, New Accreditation Heerlen: Cogito! , ''Ask me Anything'' from a Female Senior Manager's perspective (Women in Technology Lunch Sessions), Get ready for relaunch, COMMIT in feedback: Make it a habit, make it TECH!, Happy birthday, Intelligent Software Engineering Services Europe! Experience our Q4 highlights, AA Webinars, Geaccepteerd: BP Training update, Technology NL Success story - Geldmaat cash2020 program delivered by Accenture, Avoid inviting hackers into your home sweet home, conversation merged query, Your latest downloads - share your feedback, Agenda Intelligent Automation FY20 kickoff session, Celebration cake - Automation Anywhere Centre of Excellence Creation Award, Magic is real, Nordhausen, Max sent you a message in Skype for Business, UiPath APG - Dashboard, Purpose Workshop | Hosted by Corporate Citizenship NL, Announcing our Greater Than Awards and Invention of the Year Winners, Slidedeck Intelligent Automation FY20 kickoff session, Toegang tot Innersource VC-POC,  You have completed COD 262 - Fundamentals of Secure Scripting, UI Path acquires Process Gold, FIESTA: Register here for the personnel party!, Congratulations to all mid-year promotes! , Revenue Operations second robot stop, Lets build our Engineering Future, How to Make a Career on a Part-Time Contract (Women in Technology Lunch Sessions) | Reminder,  Your KLM boarding pass(es) for your flight(s) on 02 Oct, GLOBAL TECHNOLOGY INNOVATION CONTEST 2019 EUROPE ATC  ANNOUNCING THE WINNERS, MALTA installation, ATC Heerlen ranks first place for its co-innovation project with APG, 092019_VICA_Asisfunctionalities.pptx, SAVE THE DATE | September 13 | ALL PERSONNEL PARTY,  Get to know Bitbucket, TAKE THE TIME FOR FEEDBACK: MAKE IT A HABIT, MAKE IT TECH!, Reimagining Technology in FY20, ''Ask me Anything'' from a Female Senior Manager's perspective (Women in Technology Lunch Sessions),  You have completed ENG 117 - Essential Information Security Program Planning, Wholl be our next EPIC innovators?,  You have completed ENG 115 - Essential System and Information Integrity, A winning week, Grow IES Conversant : Wrapping up 2019. Welcome 2020!, Social media to the ResQ, Expand your career opportunities abroad, Recording Now Available! Ready, Set, Accenture Broadcast Featuring Amy Fuller and Paul Daugherty, FORM for Tech: Keep up the pace!, Employee satisfaction survey (Advanced Technology Center Heerlen) , Announcing our Next Chief Executive Officer, NEWSFLASH: NEW EPISODE OF PRIMETIME IS OUT | WATCH NOW, Can your post help hackers? Brand-new Hacker Land video!, Upcoming SAFe and Agile trainings - Accenture Academy, OnePager Michael Wijshoff, Let's Build Our Engineering Future, CIO Tech Works: Connect with Group and 1:1 Chats, prioritize on the People mobile app, keep passwords secure with a password manager, more, Reflection & Aspirations: Prepare Talent Discussion, CC work alignment and next steps, Campus Newsletter SSC nr. 156, October 2019, ENGAGE in feedback: Make it a habit, make it TECH!, Work from Home, Stay Future Ready - Learn Python!, CIO Asks: Do you have the right technology to do your job?, No wipeouts on this Wave, Dornostup, Yulia shared the folder \"01. AA Foundation Training\" with you., Important - 'Information Security Advocate  Program\"- by August 31, VEON - Transition to Service Center, Nordhausen, Max sent you a message in Skype for Business, Campus Newsletter SSC nr. 150, August 2019, Your latest downloads - share your feedback, Happy birthday, Intelligent Software Engineering Services!, Discuss Dashboard Trigger Solution, New Now News: taking the pole position in innovation, putting CEOs in the hot seat, and protecting our intellectual property, BP Exam, ''Ask me Anything'' from a Female Senior Manager's perspective (Women in Technology Lunch Sessions),\n",
      "\n",
      "Cluster 3 words: b'iq', b'bots', b'heerlen', b'date', b'days', b'tell',\n",
      "Files belonging to Cluster 3: IQ Bot training in Heerlen!, IQ Bot training in Heerlen!, IQ Bot training, IQ Bot training in Heerlen!,\n",
      "\n",
      "Cluster 4 words: b'request', b'email', b'actions', b'documents', b'support', b'veon',\n",
      "Files belonging to Cluster 4:  Mobile phone expenses for July 2019 have been generated., HOLIDAYS: Christmas 2019, Your Request RITM5163653 for Application Support has comments added, Help with implementation Azure AD RITM5007976, End User Digest: 1 New Message ,  Update onepager for onboarding Accenture, PA - User Accounts_cas.van.sebillen_mahmoud.allouh_andrea.feher_zandra.lundegard, End User Digest: 1 New Message , Your Request RITM5163653 for Application Support has comments added, CDP RORO ( Roll On ) Compliance, Take Action by June 24: Install mandatory Symantec Endpoint Protection version, Training approval is pending,  Boarding pass for Ms Andrea Feher BA0375:TLS-LHR:02-OCT-2019, HOURS October, IMMEDIATE ACTION REQUIRED: Accenture Data Privacy Training,  Welcome to myCWT,  Pearson VUE Web Account Confirmation, One or more documents from Accenture BV await your signature, VEON - Documentation Status Email to Project, VEON - Planning and Status, IMMEDIATE ACTION REQUIRED: Accenture Data Privacy Training, Question regarding the VICA chatbot, Your signed documents from Accenture B.V., Help with implementation Azure AD RITM5007976, Roll On Confirmation, PA - User Accounts, Action Required to Retain Access, ACTION REQUIRED to comply with Austrian Authorities, VEON Login: Confirmed, VEON,  Trip document (e-ticket receipt) for Ms Andrea FEHER on 01 Oct 19 - Trip to Toulouse Blagnac, France - (KPCINB),  One time password, ACTION REQUIRED to comply with Austrian Authorities, Hacker attacks never stop! Brand-new Hacker Land!,  Confirmation of changes to Booking Reference:  K5OGZY,  Welcome to myCWT - Please verify your email address, Training approval is pending, FYI: An Update Regarding Your Enrollment Request for Vendor CBT, End User Digest: 2 New Messages , VEON Chatbot, ACTION REQUIRED to comply with Austrian Authorities,\n",
      "\n",
      "Cluster 5 words: b'iasc', b'friday', b'snow', b'andrea', b'user', b'blue',\n",
      "Files belonging to Cluster 5: IASC - SNOW User - Andrea Feher, INC0921518: IASC - SNOW User - Andrea Feher, IASC & Priorities!, IASC - Email Box , IASC Stand-up: Blue, IASC - Christmas Planning, IASC & Priorities!, INC0921518 - IASC - SNOW User - Andrea Feher, Friday morning, Downtime Alert: Friday, July 12th, 2019,\n",
      "\n",
      "Cluster 6 words: b'report', b'time', b'andrea', b'r', b'support', b'myte',\n",
      "Files belonging to Cluster 6: AWARENESS! Time Report Submission for Andrea Fehr, 2019/07/31, Important FY20 MyTE information - Time Report Due by 12th, AWARENESS! Time Report Submission for Andrea Fehr, 2019/08/15, Important FY19 MyTE information - Time Report Due by 28th, AWARENESS! Time Report Submission for Andrea Fehr, 2019/09/30, Summary Tab Screenshot, Upgrade your iOS device to 12.3.1 or higher, AWARENESS! Time Report Submission for Andrea Fehr, 2019/08/31, Important FY19 MyTE information - Time Report Due by 12th, Time Report - WBS, AA - Training Course,\n",
      "\n",
      "Cluster 7 words: b'cancelling', b'school', b'analyst', b'session', b'days', b'starting',\n",
      "Files belonging to Cluster 7: Welcome to Design Thinking Online (DTO), FYI: An Update Regarding Your Enrollment Request for NL Technology Analyst School, Welcome to Accenture Secure Application Development Program, R: Dropping out of the Tech Analyst School, Netherlands Training Offering in Q1, Training availability Netherlands (September 23rd - November 12th)  You are invited to register!, Training availability in Amsterdam, Technology Analyst School - confirmation and logistics, Cancellation Request Received for MALTA Functionalities and Use Cases, Welcome to NL Technology Analyst School, Training availability Netherlands (November 4th - December 10th)  You are invited to register!, FYI: Learning Activity Reminder for NL Technology Analyst School, Reminder to Answer NLTAS3 April 2019 Evaluation - Copy, Reminder to Answer NLTAS3 April 2019 Evaluation - Copy, Hours for October, Welcome to AI Hackathon  Introduction to Intelligent Case Processing and MALTA, Canceled: APG reporting UiPath/PowerBI, Training availability in Amsterdam, Analyst School Feedback, Training availability Netherlands (November 4th - December 10th)  You are invited to register!,\n",
      "\n",
      "Cluster 8 words: b'join', b'meet', b'summer', b'video', b'microsoft', b'heerlen',\n",
      "Files belonging to Cluster 8: IASC FY'20, Mapathon Tomorrow at 17:00-18:30 in common area!, FRIENDLY REMINDER: Join us for the Business Update Summer Edition, RPA/IA Team Meeting, Youre invited to join the 2020 Accenture Heerlen & Salesforce SKITRIP! , RPA Meeting, Canceled: Routine/Standard Accruals J.E. update, Intelligent Automation Career Paths Walkthrough, VEON - Transition to Service Center, Routine/Standard Accruals J.E. update, Routine/Standard Accruals J.E. update, VOILA Chatbot, VEON - Transition to Service Center, >> 3 spots left >> Youre invited to join the 2020 Accenture Heerlen & Salesforce SKITRIP! , Mapathon Tomorrow at 17:00-18:30 in common area!, SC: Implementation Stand-up, Automatic reply: Join us for the Business Update Summer Edition, Canceled: VOILA Chatbot,\n",
      "\n",
      "Cluster 9 words: b'team', b'microsoft', b'chat', b'sent', b'cio', b'workshop',\n",
      "Files belonging to Cluster 9: You have been added to a team in Microsoft Teams, You have been added to a team in Microsoft Teams, Join a free CIO Virtual Workshop on Microsoft Teams, Planner or New Joiner!, FY20 - Team Presentation, You have been added to a team in Microsoft Teams, CIO Tech Works: Three dos and don'ts on sharing info in Microsoft Teams, new Hacker Land video, stay safe with the Protect myTech tool and more, Michael and Remy sent 2 messages to your chat, Rachelle sent a message, Learn how to get started with Microsoft Teams, Surface Hub and more!, You have been added to a team in Microsoft Teams,\n",
      "\n",
      "Cluster 10 words: b'wbs', b'access', b'myte', b'andrea', b'desk', b'rpa',\n",
      "Files belonging to Cluster 10: RPA Automated Service Desk - Status, WBS for flight ATR, Tech Analyst School 7-11 October, Access mailbox, WBS - Andrea Hours, MyTE - WBS, WBS Access, WBS Sparkasse, WBS for flight ATR, RPA Automated Service Desk - Status, WBS Sparkasse, MyTE - WBS,\n",
      "\n",
      "Cluster 11 words: b'dutch', b'bvc', b'berlitz', b'r', b'pr1336092', b'eur',\n",
      "Files belonging to Cluster 11: Notification: Balaji B. Mohan approved the requisition - PR1336092 - Andrea Fehr - Berlitz - BVC 40 lessons (semi-)individual - Dutch (1,843.00 EUR),  Proposal Berlitz BVC Dutch, Notification: Watch the requisition that Darina Stavarache submitted on your behalf - PR1336092 - Andrea Fehr - Berlitz - BVC 40 lessons (semi-)individual - Dutch (1,843.00 EUR), Notification: Corey J. Brennan approved the requisition - PR1336092 - Andrea Fehr - Berlitz - BVC 40 lessons (semi-)individual - Dutch (1,843.00 EUR), R: Dutch Course, Notification: Corey J. Brennan approved the requisition - PR1336092 - Andrea Fehr - Berlitz - BVC 40 lessons (semi-)individual - Dutch (1,843.00 EUR), Notification: Corey J. Brennan approved the requisition - PR1336092 - Andrea Fehr - Berlitz - BVC 40 lessons (semi-)individual - Dutch (1,843.00 EUR),\n",
      "\n",
      "Cluster 12 words: b'atr', b'kahoot', b'mail', b'quiz', b'client', b'answer',\n",
      "Files belonging to Cluster 12: ATR: Mandatory Info Client Visit,  A Kahoot has been shared with you, Uniform trainings | Centralized Kahoot quizzes, ATR: Mail Estimate, ATR - MD&I Cockpit, [NBO] Scenario 2 - Impact on SLA calculation, Reminder: Kahoot Quiz Questions, Activities, Automatisch antwoord: Kahoot Quiz Questions, ATR: Mandatory Info Client Visit, ATR: Mail Estimate,  NBO Automation - ACN guys on Wednesday, Kahoot Quiz Correction, ATR: Mandatory Info Client Visit, Mailbox Support_Robotics, ATR - NLP, Kahoot Quiz Questions,\n",
      "\n",
      "... Grid ...\n",
      "C = 10 , Iterations = 10000\n",
      "... Support Vector Machine doing its magic ...\n",
      "File saved!\n"
     ]
    }
   ],
   "source": [
    "import_train_test('C:/Users/andrea.feher/Downloads/', 'andrea.feher@accenture.com', \"Inbox\", 2019, 6, 1, 1, 1, 1,\n",
    "    {'kahoots' : ['kahoot', 'quiz', 'answers', 'questions', 'correct', 'account'], \n",
    "    'courses/learning boards' : ['learning board', 'course', 'training', 'exam', 'program', 'register', 'skill', 'completed', 'transcript', 'curriculum', 'accreditation'],  \n",
    "    'meetings' : ['meeting', 'event', 'attend', 'rpa team', 'lunch', 'join', 'session', 'party'], \n",
    "    'myTE' : ['myte', 'wbs', 'roll', 'myt&e', 'submit', 'submission', 'hours', 'unassigned', 'chargeable'], \n",
    "    'messages' : ['skype', 'microsoft teams', 'video', 'join', 'conversation', 'message'],\n",
    "    'accenture spam': ['action', 'newsletter', 'gallia', 'feedback', 'take action', 'habit', 'conversant', 'trending', 'award', 'versed', 'tech'],\n",
    "    'veon' : ['chatbot', 'vica', 'veon', 'access', 'azure', 'account', 'documentation', 'code'], \n",
    "    'atr' : ['nlp', 'atr', 'dispatcher', 'malta', 'robot', 'parts', 'spares', 'quotation', 'estimate'], \n",
    "    'travelling' : ['passenger', 'boarding', 'travel', 'hotel', 'ticket', 'booking', 'terminal', 'flight'],    \n",
    "    'other' : [\"password\"]},\n",
    "    ['email', 'emails', 'accenture', 'thank', 'thanks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kahoot, quiz, answers, questions, correct, account\n",
    "learning board, course, training, exam, program, register, skill, completed, transcript, curriculum, accreditation\n",
    "meeting, canceled, event, cancelled, meeting, attend, rpa team, lunch, join, session, party\n",
    "myte, wbs, roll, myt&e, submit, submission, hours, unassigned, chargeable\n",
    "skype, microsoft teams, message, join, conversation, video\n",
    "newsletter, action, gallia, feedback, take action, habit, conversant, trending, award, versed, tech\n",
    "passenger, boarding, travel, hotel, ticket, booking, terminal, flight\n",
    "chatbot, vica, veon, access, azure, account, documentation, code\n",
    "nlp, atr, dispatcher, malta, robot, parts, spares, quotation, estimate\n",
    "password\n",
    "\n",
    "\n",
    "email, emails, accenture, thank, thanks\n",
    "\n",
    "\n",
    "urgent\n",
    "incident/change tickets\n",
    "credentials\n",
    "pop-up\n",
    "infrastructure\n",
    "meetings\n",
    "reports\n",
    "SLA_warning\n",
    "documentation\n",
    "lifecycle\n",
    "other\n",
    "\n",
    "p1, urgent, important, asap, high priority, priority, p2, as soon as possible, stopped, priority 1, prompt, urgent:, important:\n",
    "description:, assigned to, automatically generated, incident, change, assigned, on your behalf, click here \n",
    "password, new, reset, maintenance, credential, login, log in, credentials, expire, user \n",
    "pop-up, popup, pop up, popping, popped, block, message, window\n",
    "connection, sap, connectivity, virtual machine, item, reboot, output, inform, fail, failing, infrastructure\n",
    "call, meet, event, meeting, meetings, call, calls, join, joining, skype, teams, schedule, tomorrow\n",
    "report, daily, snow, performance, process, service-now, complete, reports, servicenow, service now, status\n",
    "sla, warning, breach, task state, robotics, task\n",
    "pdd, tdd, sdd, share, documentation, fdd, change, file, manual, brd, update, review, design document \n",
    "handover, lifecycle, go-live, go live, test, testing, knowledge transfer, hand-over\n",
    "out of office, office \n",
    "        \n",
    "robotics, issue, run, runs, flight, flights, running, case, case, robots, errors, rpa, nice, process, processing, processes, processed, email, emails, thank, thanks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime.strptime(\"21/11/06 16:30\", \"%d/%m/%y %H:%M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2006, 11, 21, 16, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
