{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import win32com.client\n",
    "import sys\n",
    "import re\n",
    "import pandas\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "import pandas as pd\n",
    "from pandas.compat import StringIO\n",
    "from langdetect import detect\n",
    "import io\n",
    "import os\n",
    "import unicodecsv as csv\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import codecs\n",
    "import pickle\n",
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import seaborn as sns\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import mpld3\n",
    "import uuid\n",
    "import datetime\n",
    "from gensim.summarization import keywords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubbish = [\"intended recipient\", \"designated recipient\", \"consider the environmental\", \"may contain confidential\", \n",
    "           \"may be confidential\", \"received this in error\", \"communication in error\", \"privileged and confidential\",\n",
    "           \"environmental impact\", \"consider the environment\",  \"before printing\", \"Before printing\", \"Twitter\",\n",
    "           \"from an EXTERNAL SENDER\", \"Sent: \", \"Cc: \", \"LinkedIn\", \"Sent from my iPhone\", \"Thank you in advance.\", \"RE: \",\n",
    "          'vriendelijke', 'Euregio Digital Delivery Center', 'information that is confidential', 'for the person or entity']\n",
    "\n",
    "def import_train_test (output_file, root_folder, folder, year, month, day, hour, minute, second, classes, spamm, rubbish = rubbish):\n",
    "    importmails(output_file, root_folder, folder, year, month, day, hour, minute, second)\n",
    "    cleanmails('C:/Users/andrea.feher/Downloads/jk_sct_train.csv', 'en')\n",
    "    cluster('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv', classes)\n",
    "    addkeywords ('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv', classes, spamm)\n",
    "    trainsvm ('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv')\n",
    "    testsvm('C:/Users/andrea.feher/Downloads/jk_sct_test.csv')\n",
    "    #remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')\n",
    "def train_test (classes, spamm):\n",
    "    cleanmails('C:/Users/andrea.feher/Downloads/jk_sct_train.csv', 'en')\n",
    "    cluster('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv', classes)\n",
    "    addkeywords ('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv', classes, spamm)\n",
    "    trainsvm ('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv')\n",
    "    testsvm('C:/Users/andrea.feher/Downloads/jk_sct_test.csv')\n",
    "    #remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')\n",
    "def just_train (classes, spamm):\n",
    "    cleanmails('C:/Users/andrea.feher/Downloads/jk_sct_train.csv', 'en')\n",
    "    cluster('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv', classes)\n",
    "    addkeywords ('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv', classes, spamm)\n",
    "    trainsvm ('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv')\n",
    "    #remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')\n",
    "def just_test ():\n",
    "    testsvmm('C:/Users/andrea.feher/Downloads/jk_sct_test1.csv')\n",
    "    #remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "\n",
    "def remove_files (folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "def importmails(output_file1, root_folder, folder, year, month, day, hour, minute, second):\n",
    "    output_file = open(output_file1,'wb')    \n",
    "    output_writer = csv.writer(output_file, delimiter = \",\", encoding='latin2', errors = 'ignore')\n",
    "    outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "    root_folder = outlook.Folders(root_folder)\n",
    "    folder = root_folder.Folders(folder)\n",
    "    ssbox = root_folder.Folders[1]\n",
    "    counter = 0\n",
    "    for folder in ssbox.Folders:\n",
    "        counter += 1\n",
    "    if counter < 2:\n",
    "        inbox = outlook.GetDefaultFolder(6)\n",
    "        messages = inbox.Items\n",
    "        message = messages.GetFirst()\n",
    "        while message:\n",
    "            try:\n",
    "                if message.SenderEmailType=='EX':\n",
    "                    sender = re.sub(r'.*-', '', message.SenderEmailAddress).lower()+'@accenture.com'\n",
    "                else:\n",
    "                    sender = message.SenderEmailAddress\n",
    "                output_writer.writerow([\n",
    "                    message.subject,\n",
    "                    sender,\n",
    "                    message.Body])\n",
    "            except Exception as e:\n",
    "                    ()\n",
    "            message = messages.GetNext()\n",
    "        output_file.close()\n",
    "    else:\n",
    "        for folder in ssbox.Folders:    \n",
    "            print(folder)\n",
    "            messages = folder.Items\n",
    "            messages = messages.Restrict(\"[ReceivedTime] >= '\" +datetime.datetime(year, month, day, hour, minute, second).strftime('%m/%d/%Y %H:%M %p')+\"'\")\n",
    "            message = messages.GetFirst()\n",
    "            print('-------------')\n",
    "            print(str(len(list(messages)))+' messages in folder ')\n",
    "            while message:\n",
    "                try:\n",
    "                    if message.SenderEmailType=='EX':\n",
    "                        sender = re.sub(r'.*-', '', message.SenderEmailAddress).lower()+'@accenture.com'\n",
    "                    else:\n",
    "                        sender = message.SenderEmailAddress\n",
    "                    output_writer.writerow([\n",
    "                        message.subject,\n",
    "                        sender,\n",
    "                        message.Body])\n",
    "                except Exception as e:\n",
    "                    ()\n",
    "                message = messages.GetNext()\n",
    "        output_file.close()\n",
    "    split(output_file1)\n",
    "\n",
    "def split(output_file1):\n",
    "    df4 = pandas.read_csv(output_file1, header=None, encoding = \"latin2\")\n",
    "    train=df4.sample(frac=0.7,random_state=1993)\n",
    "    test=df4.drop(train.index)\n",
    "    test2=test.sample(frac=0.99,random_state=1993)\n",
    "    test3=test.drop(test2.index)\n",
    "    train.to_csv(r'C:/Users/andrea.feher/Downloads/jk_sct_train.csv',header=None, index=False)\n",
    "    test2.to_csv(r'C:/Users/andrea.feher/Downloads/jk_sct_test.csv',header=None, index=False)\n",
    "    test3.to_csv(r'C:/Users/andrea.feher/Downloads/jk_sct_test1.csv',header=None, index=False)\n",
    "    \n",
    "def removepersonalnames(text):\n",
    "    document_big = nlp_big(text)\n",
    "    entities_big = [e.string for e in document_big.ents if 'PERSON'==e.label_] \n",
    "    entities_big = list(entities_big) \n",
    "    for i in entities_big:\n",
    "        if i in text:\n",
    "            text = text.replace(i, '')\n",
    "    return text\n",
    "\n",
    "def cleanmails(output_file, lang):\n",
    "    try:\n",
    "        print( )\n",
    "        print('... Initiating the cleaning process ...')\n",
    "        df = pandas.read_csv(output_file, header=None, encoding = \"latin2\")\n",
    "        sep1 = '< > > wrote:'\n",
    "        sep2 = ' > het volgende geschreven:'\n",
    "        oldreplacebody = ('\\r', '\\t','\\n \\n','\\n\\n\\n\\n','\\n\\n\\n','\\n\\n')\n",
    "        newreplacebody = ('','',\"\\n\",\"\\n\",\"\\n\",\"\\n\")\n",
    "        oldreplacesub = ('[External]', 'FW: ', 'Fw: ', 'Fwd: ', 'RE: ', 'Re: ')\n",
    "        newreplacesub = ('','','', '','', '')\n",
    "        for i,j in df.iterrows():\n",
    "            j[2] = str(j[2])\n",
    "            j[2] = re.sub('<[^>]+>', '< >', j[2], flags=re.MULTILINE)\n",
    "            j[2] = re.sub('To:[^>]+\\n', '', j[2], flags=re.MULTILINE)\n",
    "            j[2] = j[2].split(sep1, 1)[0]\n",
    "            j[2] = j[2].split(sep2, 1)[0]\n",
    "            for check, rep in zip(oldreplacebody, newreplacebody):\n",
    "                j[2] = j[2].replace(check, rep)\n",
    "            j[0] = str(j[0])    \n",
    "            for check, rep in zip(oldreplacesub, newreplacesub):\n",
    "                j[0] = j[0].replace(check, rep)\n",
    "        df.rename(columns={0:'subject', 1:'sender_address', 2: \"body\"}, inplace=True)\n",
    "        corpus, ID = [], []\n",
    "        global cellist\n",
    "        cellist = []\n",
    "        for i,j in df.iterrows():\n",
    "            ID.append(str(uuid.uuid4()))   \n",
    "        df['ID'] = ID\n",
    "        for i,j in df.iterrows():\n",
    "            corpus.append(j['body'])\n",
    "        length = len(df)\n",
    "        if len(df) == len(corpus):\n",
    "            print('The total number of files to clean is ', str(len(df)))\n",
    "        global nlp_big\n",
    "        nlp_big =  spacy.load('en_core_web_md')\n",
    "        if length > 1:\n",
    "            if length > 1000:\n",
    "                counter = round(length/50)\n",
    "            else:\n",
    "                counter = round(length/10)\n",
    "            for r in range(0, len(corpus)):\n",
    "    #            corpus[r] = removepersonalnames(corpus[r])\n",
    "                convert(corpus[r])\n",
    "                perc = r * 100/length\n",
    "                rounded = \"{0:.2f}\".format(perc)\n",
    "                if r % counter == 0:\n",
    "                    print('... Approximately', rounded, ' % of your files are clean, currently cleaning file no. ', len(cellist), '...')\n",
    "        else:\n",
    "                counter = round(length/10)\n",
    "                for r in range(0, len(corpus)):\n",
    "                    corpus[r] = removepersonalnames(corpus[r])\n",
    "                    convert(corpus[r])\n",
    "        df3 = pandas.read_csv('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk2.csv', header=None, encoding = \"latin2\")\n",
    "        print('... Filtering emails in English ...')\n",
    "        df.index = df3.index\n",
    "        df3['sender_address'] = df['sender_address']\n",
    "        df3['subject'] = df['subject']\n",
    "        df3['body'] = df['body']\n",
    "        df3['ID'] = df['ID']\n",
    "        df3.rename(columns={0:'corpus'}, inplace=True)\n",
    "        df3['corpus'] = df3['corpus'].map(str) + '\\n' + df3['subject'] + \" \" + df3['subject'] + \" \" + df3['subject']\n",
    "        lang1 = []\n",
    "        for i,j in df3.iterrows():\n",
    "            j['corpus'] = str(j['corpus'])\n",
    "            if len(j['corpus']) > 100:\n",
    "                l = detect(j['corpus'])\n",
    "            else:\n",
    "                l = \"email too short\"\n",
    "            lang1.append(l)\n",
    "        df3['language'] = lang1\n",
    "        df4 = df3.loc[df3.language == str(lang)]\n",
    "        df4.to_csv(r'C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv', index=False)\n",
    "        print ()\n",
    "        print ('CLEANING COMPLETE')\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')\n",
    "\n",
    "def convert(text, threshold=.95):\n",
    "    pos_tagger = English()\n",
    "    original_email = text\n",
    "    sentences = _corpus2sentences(original_email)\n",
    "    _generate_text(sentences)\n",
    "\n",
    "def _corpus2sentences(text):\n",
    "    return text.strip().split('\\n')\n",
    "\n",
    "def _generate_text(sentences, threshold=0.95):\n",
    "    sen = '\\n'\n",
    "    clean = \"\"\n",
    "    for sentence in sentences: \n",
    "        if _prob_block(sentence, nlp_big) < threshold:\n",
    "            if not any(r in sentence for r in rubbish):\n",
    "                if clean == '':\n",
    "                    clean = sen + sentence\n",
    "                else:\n",
    "                    clean = clean + sen + sentence\n",
    "    _generate_list(clean)\n",
    "\n",
    "def _generate_list(clean):\n",
    "    cellist.append(clean)\n",
    "    with open('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk2.csv', 'wb') as myfile:\n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        for h in cellist:\n",
    "            wr.writerow([h])\n",
    "    \n",
    "def _prob_block(sentence, pos_tagger):   \n",
    "    doc = pos_tagger(sentence)\n",
    "    verb_count = np.sum([token.pos_ != \"VERB\" for token in doc])\n",
    "    length = float(len(doc))\n",
    "    thing = 0\n",
    "    if verb_count > 0:\n",
    "        thing = verb_count/length\n",
    "    else:\n",
    "        thing == 0.0\n",
    "    return thing\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "def make_corpus(dataframe):\n",
    "    corpus = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        corpus.append(j['corpus'])\n",
    "    return corpus\n",
    "\n",
    "def make_subject(dataframe):\n",
    "    subject = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        subject.append(j['subject'])\n",
    "    return subject\n",
    "\n",
    "def make_ID(dataframe):\n",
    "    ID = []\n",
    "    for i,j in dataframe.iterrows():\n",
    "        ID.append(j['ID'])\n",
    "    return ID\n",
    "\n",
    "def unique(element, listt):\n",
    "    newlist = []\n",
    "    for l in listt:\n",
    "        newlist.append(l)\n",
    "    return listt.count(element) == 1\n",
    "\n",
    "def cluster (filename, dictionary):\n",
    "    try:\n",
    "        print()\n",
    "        print('... Clustering into your ' + str(len(dictionary)) + ' topics according to the keywords given ...')\n",
    "        df = pd.read_csv(filename, encoding = \"latin2\")\n",
    "        corpus = make_corpus(df)\n",
    "        subject = make_subject(df)\n",
    "        ID = make_ID(df)\n",
    "        topicc = []\n",
    "        for j in range(0, len(corpus)):\n",
    "            corpus[j] = corpus[j].lower()\n",
    "            readlines = corpus[j].strip().split('\\n')\n",
    "            last = readlines[-1]\n",
    "            topic = []\n",
    "            for topics in dictionary.keys():\n",
    "                topic.append(topics)\n",
    "                top, top2 = [], []\n",
    "                for values in dictionary.values():\n",
    "                    counter = 0\n",
    "                    for v in values:\n",
    "                        if values.index(v) <= 2:\n",
    "                            counter += (last.count(v)) * 3\n",
    "                    top.append(counter)\n",
    "            if not all(x == 0 for x in top) and top[0] >= 2:\n",
    "                topicc.append(topic[0])\n",
    "            if not all(x == 0 for x in top) and top[0] == 0:\n",
    "                highest_index = top.index(max(top))\n",
    "                if unique(max(top), top) == True:\n",
    "                    topicc.append(topic[highest_index])\n",
    "                else:\n",
    "                    for values in dictionary.values():\n",
    "                        counter = 0\n",
    "                        for v in values:\n",
    "                            if values.index(v) == 0:\n",
    "                                counter += (corpus[j].count(v)) * 3\n",
    "                            if values.index(v) >= 1 and values.index(v) <= 2:\n",
    "                                counter += (corpus[j].count(v)) * 2\n",
    "                            else:\n",
    "                                counter += corpus[j].count(v)\n",
    "                        top2.append(counter)\n",
    "                    print(top2)\n",
    "                    if all(x == 0 for x in top2) :\n",
    "                        topicc.append('other')\n",
    "                    else:\n",
    "                        highest_index = top2.index(max(top2))\n",
    "                        if max(top2) > 2:\n",
    "                            topicc.append(topic[highest_index])\n",
    "                        else:\n",
    "                            topicc.append('other')\n",
    "            if all(x == 0 for x in top):\n",
    "                for values in dictionary.values():\n",
    "                    counter = 0\n",
    "                    for v in values:\n",
    "                        if values.index(v) == 0:\n",
    "                            counter += (corpus[j].count(v)) * 3\n",
    "                        if values.index(v) >= 1 and values.index(v) <= 2:\n",
    "                            counter += (corpus[j].count(v)) * 2\n",
    "                        else:\n",
    "                            counter += corpus[j].count(v)\n",
    "                    top2.append(counter)\n",
    "                print(top2)\n",
    "                if all(x == 0 for x in top2):\n",
    "                    topicc.append('other')\n",
    "                else:\n",
    "                    highest_index = top2.index(max(top2))\n",
    "                    if max(top2) > 2:\n",
    "                        topicc.append(topic[highest_index])\n",
    "                    else:\n",
    "                        topicc.append('other')\n",
    "        df['topic'] = topicc\n",
    "        df.to_csv(r'C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete_labeled.csv', index=False)\n",
    "        return df\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')\n",
    "\n",
    "def addkeywords (filename, dictionary, spam):\n",
    "    df = pd.read_csv(filename, encoding = \"latin2\")\n",
    "    df['topic'].unique()\n",
    "    topicss = list(df['topic'].unique())\n",
    "    counter = 0\n",
    "    for j in topicss:\n",
    "        counter += 1\n",
    "        print( )\n",
    "        print('... Processing topic', j, '...')\n",
    "        df1 = df[df.topic == j]\n",
    "        corpus = make_corpus(df1)\n",
    "        k = []\n",
    "        text = ''\n",
    "        for i in corpus:\n",
    "            i = i.encode('latin2','replace').lower()\n",
    "            k.append(i)\n",
    "        text = b''.join(k)\n",
    "        values = keywords(text=text,split='\\n',scores=True)\n",
    "        data = pd.DataFrame(values,columns=['keyword','score'])\n",
    "        data = data.sort_values('score',ascending=False)\n",
    "        key_word_list = data.head(40)\n",
    "        keywordlist = list(key_word_list['keyword'])\n",
    "        for i in keywordlist:\n",
    "            if i in spam:\n",
    "                keywordlist.remove(i)\n",
    "        for i in keywordlist:\n",
    "            if i in dictionary[j]:\n",
    "                keywordlist.remove(i)\n",
    "        dictionary[j].extend(keywordlist)\n",
    "        print('The recommended keywords for this topic are the following:', dictionary[j])\n",
    "        \n",
    "def gettfidf(trainDF, d, count_vect, tfidf_vect):\n",
    "    print('... Transforming the validation data ...')\n",
    "    count = count_vect.transform(d)\n",
    "    tfidf_vect.fit(trainDF['corpus'])\n",
    "    tfidf = tfidf_vect.transform(d)\n",
    "    return tfidf   \n",
    "\n",
    "def gettfidf2(trainDF, d, count_vect, tfidf_vect):\n",
    "    print('... Transforming the training data ...')\n",
    "    count = count_vect.transform(d)\n",
    "    tfidf_vect.fit(trainDF['corpus'])\n",
    "    tfidf1 = tfidf_vect.transform(d)\n",
    "    return tfidf1\n",
    "\n",
    "def tdf (data):\n",
    "    labels, texts, ID = [], [], []\n",
    "    for i, line in data.iterrows():\n",
    "        labels.append(line['topic'])\n",
    "        texts.append(line['corpus'])\n",
    "        ID.append(line['ID'])\n",
    "    print('... Creating a dataframe using just texts and labels ...')\n",
    "    trainDF = pandas.DataFrame()\n",
    "    trainDF['corpus'] = texts\n",
    "    trainDF['topic'] = labels\n",
    "    trainDF['ID'] = ID\n",
    "    return trainDF\n",
    "\n",
    "def trainsvm (file):\n",
    "    try:\n",
    "        print( )\n",
    "        print('... Processing the training set ...')\n",
    "        data = pandas.read_csv(file, encoding = \"latin2\")\n",
    "        trainDF = tdf (data)\n",
    "        train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['corpus'], trainDF['topic'], \n",
    "                                                                              test_size=0.2, random_state=1993)\n",
    "        topicsinwords = list(trainDF['topic'].unique())\n",
    "        topicsinwords = sorted(topicsinwords)\n",
    "        print('... Encoding the topic number ...')\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        train_y = encoder.fit_transform(train_y)\n",
    "        valid_y = encoder.fit_transform(valid_y)\n",
    "        topicsinnumbers = list(set(list(train_y)))\n",
    "        count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', decode_error=\"replace\")\n",
    "        xtrain_count = count_vect.fit_transform(train_x)\n",
    "        pickle.dump(count_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/feature.pkl\",\"wb\"))\n",
    "        xvalid_count = count_vect.transform(valid_x)\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=1000)\n",
    "        tfidf_vect.fit(trainDF['corpus'])\n",
    "        pickle.dump(tfidf_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/features.pkl\",\"wb\"))\n",
    "        xtrain_tfidf = gettfidf(trainDF, train_x, count_vect, tfidf_vect)\n",
    "        xvalid_tfidf = gettfidf2(trainDF, valid_x, count_vect, tfidf_vect)\n",
    "        print('... Fitting the training dataset on the classifier ...')\n",
    "        Cs = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "        max_iter = [1, 10, 1000, 100, 10000]\n",
    "        param_grid = {'C': Cs, 'max_iter' : max_iter}\n",
    "        grid_search = GridSearchCV(svm.LinearSVC(), param_grid, cv = 3)\n",
    "        print('... Grid ...')\n",
    "        grid_search.fit(xtrain_tfidf, train_y)\n",
    "        C = grid_search.best_params_['C']\n",
    "        iterations = grid_search.best_params_['max_iter']\n",
    "        print('C =', C, ', Iterations =', iterations)\n",
    "        model = LinearSVC(C=C, max_iter = iterations)\n",
    "        model.fit(xtrain_tfidf, train_y)\n",
    "        valid_predictions_SVM = model.predict(xvalid_tfidf)\n",
    "        print(\"SVM Accuracy Score -> \",accuracy_score(valid_predictions_SVM, valid_y)*100)\n",
    "        check = pandas.DataFrame()\n",
    "        check['pred'] = valid_predictions_SVM\n",
    "        check['actual'] = valid_y\n",
    "        check['Index'] = valid_x.index\n",
    "        check.index = valid_x.index\n",
    "        p = list(check.index)\n",
    "        rowData = data.iloc[p, : ]\n",
    "        rowData.index = check.index\n",
    "        rowData['pred'] = check['pred']\n",
    "        rowData['pred'] = encoder.inverse_transform(rowData['pred'])\n",
    "        print(rowData[['subject', 'topic', 'pred']].head(30))\n",
    "        print(\"... Saving the tfidf dataframe ...\")\n",
    "        tfidfdf = pandas.DataFrame(xtrain_tfidf.toarray())\n",
    "        tfidfdf['label'] = train_y\n",
    "        tfidfdf.to_csv(r'C:/Users/andrea.feher/Downloads/tfidf.csv', encoding = \"latin2\")\n",
    "        labelencoding = pandas.DataFrame()\n",
    "        labelencoding['topic_number'] = topicsinnumbers\n",
    "        labelencoding['topic_name'] = topicsinwords\n",
    "        labelencoding.to_csv(r'C:/Users/andrea.feher/Downloads/labeling.csv', encoding = \"latin2\")\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')\n",
    "\n",
    "def calculate_wcss(data):\n",
    "    wcss = []\n",
    "    for n in range(2, 21):\n",
    "        kmeans = KMeans(n_clusters=n)\n",
    "        kmeans.fit(X=data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "    return wcss\n",
    "\n",
    "def optimal_number_of_clusters(wcss):\n",
    "    x1, y1 = 2, wcss[0]\n",
    "    x2, y2 = 20, wcss[len(wcss)-1]\n",
    "    distances = []\n",
    "    for i in range(len(wcss)):\n",
    "        x0 = i+2\n",
    "        y0 = wcss[i]\n",
    "        numerator = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)\n",
    "        denominator = sqrt((y2 - y1)**2 + (x2 - x1)**2)\n",
    "        distances.append(numerator/denominator)\n",
    "    return distances.index(max(distances)) + 2\n",
    "        \n",
    "def testsvm (output_file):\n",
    "    try:\n",
    "        print( )\n",
    "        print('... Initiating testing ...')\n",
    "        cleanmails (output_file, 'en')\n",
    "        df7 = pandas.read_csv('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv', encoding = \"latin2\")\n",
    "        df8 = pandas.read_csv('C:/Users/andrea.feher/Downloads/labeling.csv', encoding = \"latin2\")\n",
    "        tfidfdf = pandas.read_csv(r'C:/Users/andrea.feher/Downloads/tfidf.csv', encoding = \"latin2\")\n",
    "        tfidfdf = tfidfdf.dropna()\n",
    "        train_y = tfidfdf['label']\n",
    "        xtrain_tfidf = tfidfdf.drop(['label'], axis=1)\n",
    "        xtrain_tfidf = xtrain_tfidf.drop(['0'], axis=1)\n",
    "        xtrain_tfidf = scipy.sparse.csr_matrix(xtrain_tfidf)\n",
    "        labels, textsss, labelno, subject, ID = [], [], [], [], []\n",
    "        textsss = make_corpus(df7)\n",
    "        subject = make_subject(df7)\n",
    "        ID = make_ID(df7)\n",
    "        for i, line in df8.iterrows():\n",
    "            labels.append(line['topic_name'])\n",
    "        for i, line in df8.iterrows():\n",
    "            labelno.append(line['topic_number'])\n",
    "        testDF = pandas.DataFrame()\n",
    "        testDF['corpus'] = textsss\n",
    "        for n in range(0, len(textsss)):\n",
    "            if type(textsss[n]) == str:\n",
    "                allwords_stemmed = tokenize_and_stem(textsss[n])\n",
    "                totalvocab_stemmed.extend(allwords_stemmed)\n",
    "                allwords_tokenized = tokenize_only(textsss[n])\n",
    "                totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "        print('... Vectorizing ...')\n",
    "        tf1 = open(\"C:/Users/andrea.feher/Downloads/feature.pkl\", 'rb')\n",
    "        tf2 = open(\"C:/Users/andrea.feher/Downloads/features.pkl\", 'rb')\n",
    "        count_vect = CountVectorizer(decode_error=\"replace\", max_features=1000, vocabulary=pickle.load(tf1))\n",
    "        xtest_count = count_vect.fit_transform(testDF['corpus'])\n",
    "        #pickle.dump(count_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/feature.pkl\",\"ab\"))\n",
    "        pickle.dump(count_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/feature.pkl\",\"wb\"))\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', decode_error=\"replace\",\n",
    "                max_features=1000, tokenizer=tokenize_and_stem, stop_words='english', ngram_range=(1,3),\n",
    "                use_idf=True, vocabulary=pickle.load(tf2))\n",
    "        xtest_tfidf = tfidf_vect.fit(testDF['corpus'])\n",
    "        xtest_tfidf = tfidf_vect.transform(testDF['corpus'])\n",
    "        #pickle.dump(tfidf_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/features.pkl\",\"ab\"))\n",
    "        pickle.dump(tfidf_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/features.pkl\",\"wb\"))\n",
    "        print('This is the shape of your tfidf matrix: ', xtest_tfidf.shape, xtrain_tfidf.shape)\n",
    "        terms = tfidf_vect.get_feature_names()\n",
    "        dist = 1 - cosine_similarity(xtest_tfidf)\n",
    "        print()\n",
    "        sum_of_squares = calculate_wcss(xtest_tfidf)\n",
    "        print('... Calculating the recommended number of topics in the dataset ...')\n",
    "        num_clusters = optimal_number_of_clusters(sum_of_squares)\n",
    "        print('According to the calculation, you have ', str(num_clusters), ' topics.')\n",
    "        print('... Grouping documents into ', str(num_clusters), 'clusters... ')\n",
    "        if len(textsss) > 500:\n",
    "            print('... This might take a while, as ', (len(textsss)), 'documents are being read ... ')\n",
    "        km = KMeans(n_clusters=num_clusters, random_state=3)\n",
    "        %time km.fit(xtest_tfidf)\n",
    "        clusters = km.labels_.tolist()\n",
    "        joblib.dump(km,  'doc_cluster.pkl')\n",
    "        km = joblib.load('doc_cluster.pkl')\n",
    "        clusters = km.labels_.tolist()\n",
    "        files = { 'title': subject, 'cluster': clusters, 'ID': ID}\n",
    "        frame = pd.DataFrame(files, index = [clusters] , columns = ['title', 'cluster', 'ID'])\n",
    "        frame['cluster'].value_counts()\n",
    "        print(\"Top terms per cluster:\")\n",
    "        print()\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "        for i in range(num_clusters):\n",
    "            print()\n",
    "            print(\"Cluster %d words:\" % i, end='')\n",
    "            for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "                print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "            print()\n",
    "            print(\"Files belonging to Cluster %d:\" % i, end='')\n",
    "            for title in frame.ix[i]['title'].values.tolist():\n",
    "                print(' %s,' % title, end='')\n",
    "            print()\n",
    "        print()\n",
    "        Cs = [0.1, 1, 10, 100, 1000, 10000]\n",
    "        max_iter = [10, 100, 1000, 10000]\n",
    "        param_grid = {'C': Cs, 'max_iter' : max_iter}\n",
    "        grid_search = GridSearchCV(svm.LinearSVC(), param_grid, cv = 3)\n",
    "        print('... Grid ...')\n",
    "        grid_search.fit(xtrain_tfidf, train_y)\n",
    "        C = grid_search.best_params_['C']\n",
    "        iterations = grid_search.best_params_['max_iter']\n",
    "        print('C =', C, ', Iterations =', iterations)\n",
    "        print('... Support Vector Machine doing its magic ...')\n",
    "        model = LinearSVC(C=C, max_iter = iterations, class_weight = 'balanced')\n",
    "        model.fit(xtrain_tfidf, train_y)\n",
    "        test_predictions_SVM = model.predict(xtest_tfidf)\n",
    "        check = pandas.DataFrame()\n",
    "        check['pred'] = test_predictions_SVM\n",
    "        rowData = df7\n",
    "        rowData.index = check.index\n",
    "        rowData['pred'] = check['pred']\n",
    "        for i in range(len(labels)):\n",
    "            rowData['pred'] = rowData['pred'].replace({labelno[i]:labels[i]})\n",
    "        rowData.to_csv(r'C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/tested-labeled.csv', index=False)\n",
    "        print('File saved!')\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')\n",
    "\n",
    "def testsvmm (output_file):\n",
    "    try:\n",
    "        print('... Classification initiated ...')\n",
    "        cleanmails (output_file, 'en')\n",
    "        df7 = pandas.read_csv('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/jk_sct_eng_complete.csv', encoding = \"latin2\")\n",
    "        df8 = pandas.read_csv('C:/Users/andrea.feher/Downloads/labeling.csv', encoding = \"latin2\")\n",
    "        tfidfdf = pandas.read_csv(r'C:/Users/andrea.feher/Downloads/tfidf.csv', encoding = \"latin2\")\n",
    "        tfidfdf = tfidfdf.dropna()\n",
    "        train_y = tfidfdf['label']\n",
    "        xtrain_tfidf = tfidfdf.drop(['label'], axis=1)\n",
    "        xtrain_tfidf = xtrain_tfidf.drop(['0'], axis=1)\n",
    "        xtrain_tfidf = scipy.sparse.csr_matrix(xtrain_tfidf)\n",
    "        labels, textsss, labelno, subject, ID = [], [], [], [], []\n",
    "        textsss = make_corpus(df7)\n",
    "        subject = make_subject(df7)\n",
    "        ID = make_ID(df7)\n",
    "        for i, line in df8.iterrows():\n",
    "            labels.append(line['topic_name'])\n",
    "        for i, line in df8.iterrows():\n",
    "            labelno.append(line['topic_number'])\n",
    "        testDF = pandas.DataFrame()\n",
    "        testDF['corpus'] = textsss\n",
    "        for n in range(0, len(textsss)):\n",
    "            if type(textsss[n]) == str:\n",
    "                allwords_stemmed = tokenize_and_stem(textsss[n])\n",
    "                totalvocab_stemmed.extend(allwords_stemmed)\n",
    "                allwords_tokenized = tokenize_only(textsss[n])\n",
    "                totalvocab_tokenized.extend(allwords_tokenized)\n",
    "        vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "        print('... Vectorizing ...')\n",
    "        tf1 = open(\"C:/Users/andrea.feher/Downloads/feature.pkl\", 'rb')\n",
    "        tf2 = open(\"C:/Users/andrea.feher/Downloads/features.pkl\", 'rb')\n",
    "        count_vect = CountVectorizer(decode_error=\"replace\", max_features=1000, vocabulary=pickle.load(tf1))\n",
    "        xtest_count = count_vect.fit_transform(testDF['corpus'])\n",
    "        #pickle.dump(count_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/feature.pkl\",\"ab\"))\n",
    "        pickle.dump(count_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/feature.pkl\",\"wb\"))\n",
    "        tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', decode_error=\"replace\",\n",
    "                max_features=1000, tokenizer=tokenize_and_stem, stop_words='english', ngram_range=(1,3),\n",
    "                use_idf=True, vocabulary=pickle.load(tf2))\n",
    "        xtest_tfidf = tfidf_vect.fit(testDF['corpus'])\n",
    "        xtest_tfidf = tfidf_vect.transform(testDF['corpus'])\n",
    "        #pickle.dump(tfidf_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/features.pkl\",\"ab\"))\n",
    "        pickle.dump(tfidf_vect.vocabulary_,open(\"C:/Users/andrea.feher/Downloads/features.pkl\",\"wb\"))\n",
    "        print('This is the shape of your tfidf matrix: ', xtest_tfidf.shape, xtrain_tfidf.shape)\n",
    "        Cs = [0.1, 1, 10, 100, 1000, 10000]\n",
    "        max_iter = [10, 100, 1000, 10000]\n",
    "        param_grid = {'C': Cs, 'max_iter' : max_iter}\n",
    "        grid_search = GridSearchCV(svm.LinearSVC(), param_grid, cv = 3)\n",
    "        print('... Grid ...')\n",
    "        grid_search.fit(xtrain_tfidf, train_y)\n",
    "        C = grid_search.best_params_['C']\n",
    "        iterations = grid_search.best_params_['max_iter']\n",
    "        print('C =', C, ', Iterations =', iterations)\n",
    "        print('... Support Vector Machine doing its magic ...')\n",
    "        model = LinearSVC(C=C, max_iter = iterations, class_weight = 'balanced')\n",
    "        model.fit(xtrain_tfidf, train_y)\n",
    "        test_predictions_SVM = model.predict(xtest_tfidf)\n",
    "        check = pandas.DataFrame()\n",
    "        check['pred'] = test_predictions_SVM\n",
    "        rowData = df7\n",
    "        rowData.index = check.index\n",
    "        rowData['pred'] = check['pred']\n",
    "        for i in range(len(labels)):\n",
    "            rowData['pred'] = rowData['pred'].replace({labelno[i]:labels[i]})\n",
    "        rowData.to_csv(r'C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/tested-labeled.csv', index=False)\n",
    "        print('File saved!')\n",
    "    except KeyboardInterrupt:\n",
    "        print('... Interrupted, removing files ...')\n",
    "        remove_files('C:/Users/andrea.feher/Downloads/Python Notebooks and Scripts/emails/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import_train_test('C:/Users/andrea.feher/Downloads/jk_sct.csv', 'support_robotics@accenture.com', \"Postvak IN\", \n",
    "                  2018, 9, 1, 0, 0, 0, \n",
    "           {'urgent' : ['p1', 'urgent', 'important', 'asap', 'high priority', 'priority','p2', \n",
    "                       'as soon as possible', 'stopped', 'priority 1', 'prompt', 'urgent:', 'important:'],\n",
    "           'incident/change tickets' : ['description: ', 'assigned to', 'automatically generated', 'incident', \n",
    "                        'change', 'assigned', 'on your behalf', 'click here'], \n",
    "           'credentials' : ['password', 'new', 'reset', 'maintenance', 'credential', 'login', 'log in', \n",
    "                        'credentials', 'expire', 'user'], \n",
    "            'pop-up' : ['pop-up', 'popup', 'pop up', 'popping', 'popped', 'block', 'message', 'window'], \n",
    "             'infrastructure' : ['connection', 'sap', 'connectivity', 'virtual machine', 'item', 'reboot', \n",
    "                        'output', 'inform', 'fail', 'failing', 'infrastructure'], \n",
    "           'meetings' : ['call', 'meet', 'event', 'meeting', 'meetings', 'call', 'calls', 'join', 'joining', \n",
    "                         'skype', 'teams', 'schedule', 'tomorrow'], \n",
    "           'reports' : ['report', 'daily', 'snow', 'performance', 'process', 'service-now', 'complete', \n",
    "                        'reports', 'servicenow', 'service now', 'status'],\n",
    "           'SLA_warning' : ['sla', 'warning', 'breach', 'task state', 'robotics', 'task'], \n",
    "           'documentation' : ['pdd', 'tdd', 'sdd', 'share','documentation', 'fdd', 'change', 'file', 'manual', \n",
    "                        'brd',  'update', 'review', 'design document'], \n",
    "           'lifecycle' : ['handover', 'lifecycle', 'go-live', 'go live', 'test', 'testing', 'knowledge transfer', \n",
    "                          'hand-over'],\n",
    "           'other' : ['out of office','office']\n",
    "          }, ['robotics', 'issue', 'run', 'runs', 'flight', 'flights', 'running', 'case', 'case', 'robots', 'errors', \n",
    "         'rpa', 'nice', 'process', 'processing', 'processes', 'processed', 'email', 'emails', 'thank', 'thanks'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_test( {'urgent' : ['p1', 'urgent', 'important', 'asap', 'high priority', 'priority','p2', \n",
    "                       'as soon as possible', 'stopped', 'priority 1', 'prompt', 'urgent:', 'important:'],\n",
    "           'incident/change tickets' : ['description: ', 'assigned to', 'automatically generated', 'incident', \n",
    "                        'change', 'assigned', 'on your behalf', 'click here'], \n",
    "           'credentials' : ['password', 'new', 'reset', 'maintenance', 'credential', 'login', 'log in', \n",
    "                        'credentials', 'expire', 'user'], \n",
    "            'pop-up' : ['pop-up', 'popup', 'pop up', 'popping', 'popped', 'block', 'message', 'window'], \n",
    "             'infrastructure' : ['connection', 'sap', 'connectivity', 'virtual machine', 'item', 'reboot', \n",
    "                        'output', 'inform', 'fail', 'failing', 'infrastructure'], \n",
    "           'meetings' : ['call', 'meet', 'event', 'meeting', 'meetings', 'call', 'calls', 'join', 'joining', \n",
    "                         'skype', 'teams', 'schedule', 'tomorrow'], \n",
    "           'reports' : ['report', 'daily', 'snow', 'performance', 'process', 'service-now', 'complete', \n",
    "                        'reports', 'servicenow', 'service now', 'status'],\n",
    "           'SLA_warning' : ['sla', 'warning', 'breach', 'task state', 'robotics', 'task'], \n",
    "           'documentation' : ['pdd', 'tdd', 'sdd', 'share','documentation', 'fdd', 'change', 'file', 'manual', \n",
    "                        'brd',  'update', 'review', 'design document'], \n",
    "           'lifecycle' : ['handover', 'lifecycle', 'go-live', 'go live', 'test', 'testing', 'knowledge transfer', \n",
    "                          'hand-over'],\n",
    "           'other' : ['out of office','office']\n",
    "          }, ['robotics', 'issue', 'run', 'runs', 'flight', 'flights', 'running', 'case', 'case', 'robots', 'errors', \n",
    "         'rpa', 'nice', 'process', 'processing', 'processes', 'processed', 'email', 'emails', 'thank', 'thanks'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test({'kahoots' : ['kahoot', 'quiz', 'answers', 'questions', 'correct', 'account'], \n",
    "    'courses/learning boards' : ['learning board', 'course', 'training', 'exam', 'program', 'register', 'skill', 'completed', 'transcript', 'curriculum', 'accreditation'],  \n",
    "    'meetings' : ['meeting', 'canceled', 'event', 'cancelled', 'meeting', 'attend', 'rpa team', 'lunch', 'join', 'session', 'party'], \n",
    "    'myTE' : ['myte', 'wbs', 'roll', 'myt&e', 'submit', 'submission', 'hours', 'unassigned', 'chargeable'], \n",
    "    'messages' : ['skype', 'microsoft teams', 'message', 'join', 'conversation', 'video'],\n",
    "    'accenture spam': ['newsletter', 'action', 'gallia', 'feedback', 'take action', 'habit', 'conversant', 'trending', 'award', 'versed', 'tech'],\n",
    "    'other' : [\"password\"]},\n",
    "    ['email', 'emails', 'accenture', 'thank', 'thanks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_train_test('C:/Users/andrea.feher/Downloads/jk_sct.csv', 'andrea.feher@accenture.com', \"Inbox\", \n",
    "    2019, 5, 30, 0, 0, 0,\n",
    "    {'kahoots' : ['kahoot', 'quiz', 'answers', 'questions', 'correct', 'account'], \n",
    "    'courses/learning boards' : ['learning board', 'course', 'training', 'exam', 'program', 'register', 'skill', 'completed', 'transcript', 'curriculum', 'accreditation'],  \n",
    "    'meetings' : ['meeting', 'event', 'attend', 'rpa team', 'lunch', 'join', 'session', 'party'], \n",
    "    'myTE' : ['myte', 'wbs', 'roll', 'myt&e', 'submit', 'submission', 'hours', 'unassigned', 'chargeable'], \n",
    "    'messages' : ['skype', 'microsoft teams', 'video', 'join', 'conversation', 'message'],\n",
    "    'accenture spam': ['action', 'newsletter', 'gallia', 'feedback', 'take action', 'habit', 'conversant', 'trending', 'award', 'versed', 'tech'],\n",
    "    'other' : [\"password\"]},\n",
    "    ['email', 'emails', 'accenture', 'thank', 'thanks'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsvmm('C:/Users/andrea.feher/Downloads/jk_sct_test1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
